{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"K-Nearest Neighbors (KNN) Definisi Algoritma K-Nearest Neighbors (KNN) algoritma yang berfungsi untuk melakukan klasifikasi suatu data berdasarkan data pembelajaran ( train data sets ), yang diambil dari k tetangga terdekatnya ( nearest neighbors ). Dengan k merupakan banyaknya tetangga terdekat. KNN ini dapat kita temukan bilamana kita sedang belajar Machine Learning. ML intinya berkaitan dengan automasi atau bagaimana sebuah machine dapat belajar dari contoh-contoh yang kita berikan. Algoritma KNN adalah salah satu yang paling sederhana dari semua algoritma machine learning . Ini hanya menghitung jarak dari titik data baru ke semua titik data pelatihan lainnya. Jarak dapat dari jenis apa pun, misalnya: Euclidean. Kemudian akan memilih titik data K-terdekat, di mana K dapat berupa bilangan bulat apa pun. Kemudian akan diberikan titik data ke kelas tempat mayoritas titik data K berada. Iris datasets Kali ini saya menggunakan Python untuk melakukan coba-coba model. Disini saya menggunakan dataset iris. Dataset ini sangat populer digunakan untuk latihan pertama (R atau python). Iris biasanya sudah tersedia didalam modul sklearn (lengkap dengan target dan feature). # code untuk install modul sklearn pip install -U scikit-learn #jika menggunakan pip yang ada di python conda install scikit-learn #jika menggunakan anaconda Terdapat 150 observasi (row) dengan feature/independent variable sebanyak 4 (Panjang sepal, lebar sepal, panjang petal, dan lebar petal). 150 observasi tersebut dibagi menjadi 50 observasi pada masing-masing spesies ( Iris setosa , Iris versicolor , dan Iris virginica ). Pada data iris, kita tidak akan menjumpai nilai null (N/A), sehingga kita tidak perlu capek-capek untuk merapikan data tersebut. KNN dengan python Langkah pertama adalah memanggil data iris yang akan kita gunakan untuk membuat KNN. Misal masing-masing target/spesies kita berikan nilai yang unik, setosa=0, versicolor=1, virginica=2. Pada gambar di bawah ini dapat dilihat jika kita menggunakan K=1 dan data baru a=[1,2.7,3.6,4.2]. a mewakili masing-masing nilai feature. Hasilnya adalah 2, yaitu Iris virginica . In [ 3 ]: from sklearn.datasets import load_iris iris = load_iris () x = iris . data y = iris . target from sklearn.neighbors import KNeighborsClassifier import numpy as np knn = KNeighborsClassifier ( n_neighbors = 1 ) #define K=1 knn . fit ( x , y ) a = np . array ([[ 1.0 , 2.7 , 3.6 , 4.2 ]]) knn . predict ( a ) Out [ 3 ]: array ([ 2 ]) Jika saya ganti nilai k=3 dengan nilai data a tidak saya rubah, maka hasil menjadi 1, yaitu Iris versicolor. In [ 8 ]: from sklearn.datasets import load_iris iris = load_iris () x = iris . data y = iris . target from sklearn.neighbors import KNeighborsClassifier import numpy as np knn = KNeighborsClassifier ( n_neighbors = 3 ) #define K=3 knn . fit ( x , y ) a = np . array ([[ 1.0 , 2.7 , 3.6 , 4.2 ]]) knn . predict ( a ) Out [ 8 ]: array ([ 1 ]) Untuk menghindari overfitting, kita dapat membagi iris dataset tadi menjadi data train dan test. Perbandingannya 80%:20%, sehingga bakal ada x buat training dan testing, begitu juga dengan \u2018y\u2019 bakal ada y buat training dan testing. Jadi dari 150 total observasi pada data iris, terdapat 120 observasi pada data train dan 30 observasi pada data testing. Script pemisahan data train dan test dapat dilihat di bawah. from sklearn.model_selection import train_test_split X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.20 ) Cross-validation Cross-validation (CV) adalah metode statistik yang dapat digunakan untuk mengevaluasi kinerja model atau algoritma dimana data dipisahkan menjadi dua subset yaitu data proses pembelajaran dan data validasi / evaluasi. Model atau algoritma dilatih oleh subset pembelajaran dan divalidasi oleh subset validasi. Selanjutnya pemilihan jenis CV dapat didasarkan pada ukuran dataset. Biasanya CV K-fold digunakan karena dapat mengurangi waktu komputasi dengan tetap menjaga keakuratan estimasi. Misal nih, data kita ada 150. Ibarat kita pake K=5, berarti kita bagi 150 data menjadi 5 partisi, isinya masing-masing 30 data. Jangan lupa, kita perlu menentukan mana yang training data dan mana yang test data. Karena perbandingannya 80:20, berarti 120 data adalah training data dan 30 sisanya adalah test data. Berdasarkan 5 partisi tadi, berarti bakal ada 4 partisi x 30 data = 120 training data. Dan sisanya ada 1 partisi test data berisi 30 data. Kemudian, experimen menggunakan data yang udah di partisi-partisi bakal diulang 5 kali (K=5). Tapi posisi partisi Test data berbeda ditiap iterasinya. Misal di iterasi pertama Test nya di posisi partisi awal, terus iterasi partisi kedua Test-nya di posisi kedua, dan seterusnya, pokonya gaboleh sama. In [ 14 ]: import numpy as np import matplotlib.pyplot as plt import pandas as pd In [ 15 ]: url = https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data # Assign colum names to the dataset names = [ sepal-length , sepal-width , petal-length , petal-width , Class ] # Read dataset to pandas dataframe dataset = pd . read_csv ( url , names = names ) In [ 18 ]: X = dataset . iloc [:, : - 1 ] . values y = dataset . iloc [:, 4 ] . values In [ 19 ]: from sklearn.model_selection import train_test_split X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.20 ) In [ 20 ]: from sklearn.preprocessing import StandardScaler scaler = StandardScaler () scaler . fit ( X_train ) X_train = scaler . transform ( X_train ) X_test = scaler . transform ( X_test ) In [ 21 ]: from sklearn.neighbors import KNeighborsClassifier classifier = KNeighborsClassifier ( n_neighbors = 5 ) classifier . fit ( X_train , y_train ) Out [ 21 ]: KNeighborsClassifier ( algorithm = auto , leaf_size = 30 , metric = minkowski , metric_params = None , n_jobs = None , n_neighbors = 5 , p = 2 , weights = uniform ) In [ 23 ]: y_pred = classifier . predict ( X_test ) In [ 25 ]: from sklearn.metrics import classification_report , confusion_matrix print ( confusion_matrix ( y_test , y_pred )) print ( classification_report ( y_test , y_pred )) [[ 11 1 0 ] [ 0 8 0 ] [ 0 0 10 ]] precision recall f1 - score support Iris - setosa 1.00 0.92 0.96 12 Iris - versicolor 0.89 1.00 0.94 8 Iris - virginica 1.00 1.00 1.00 10 micro avg 0.97 0.97 0.97 30 macro avg 0.96 0.97 0.97 30 weighted avg 0.97 0.97 0.97 30 Hasil running code diatas menunjukkan bahwa algoritma KNN yang telah kita lakukan dapat mengklasifikasikan semua observasi (30) dalam set test dengan akurasi 97%, yang artinya susdah sangat bagus. Meskipun algoritma KNN dilakukan sangat baik dengan dataset ini, jangan berharap hasil yang sama dengan semua dataset. KNN tidak selalu berkinerja baik dengan data yang memiliki dimensi tinggi atau yang memiliki feature yang sangat kategoris. Menentukan nilai K Nah sekarang kita akan mencari tahu nilai K berapa yang akan menghasilkan akurasi tinggi. Karena kita tahu sendiri pada algoritma KNN, penentuan nilai K sangatlah krusial. Kita akan melihat nilai K dari 1\u201340. Berikut code yang dapat kita gunakan: In [ 14 ]: import numpy as np import matplotlib.pyplot as plt import pandas as pd In [ 15 ]: url = https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data # Assign colum names to the dataset names = [ sepal-length , sepal-width , petal-length , petal-width , Class ] # Read dataset to pandas dataframe dataset = pd . read_csv ( url , names = names ) In [ 18 ]: X = dataset . iloc [:, : - 1 ] . values y = dataset . iloc [:, 4 ] . values In [ 19 ]: from sklearn.model_selection import train_test_split X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.20 ) In [ 20 ]: from sklearn.preprocessing import StandardScaler scaler = StandardScaler () scaler . fit ( X_train ) X_train = scaler . transform ( X_train ) X_test = scaler . transform ( X_test ) In [ 21 ]: from sklearn.neighbors import KNeighborsClassifier classifier = KNeighborsClassifier ( n_neighbors = 5 ) classifier . fit ( X_train , y_train ) Out [ 21 ]: KNeighborsClassifier ( algorithm = auto , leaf_size = 30 , metric = minkowski , metric_params = None , n_jobs = None , n_neighbors = 5 , p = 2 , weights = uniform ) In [ 23 ]: y_pred = classifier . predict ( X_test ) In [ 26 ]: from sklearn.metrics import classification_report , confusion_matrix print ( confusion_matrix ( y_test , y_pred )) print ( classification_report ( y_test , y_pred )) [[ 11 1 0 ] [ 0 8 0 ] [ 0 0 10 ]] precision recall f1 - score support Iris - setosa 1.00 0.92 0.96 12 Iris - versicolor 0.89 1.00 0.94 8 Iris - virginica 1.00 1.00 1.00 10 micro avg 0.97 0.97 0.97 30 macro avg 0.96 0.97 0.97 30 weighted avg 0.97 0.97 0.97 30 In [ 30 ]: error = [] # Calculating error for K values between 1 and 40 for i in range ( 1 , 40 ): knn = KNeighborsClassifier ( n_neighbors = i ) knn . fit ( X_train , y_train ) pred_i = knn . predict ( X_test ) error . append ( np . mean ( pred_i != y_test )) In [ 31 ]: plt . figure ( figsize = ( 12 , 6 )) plt . plot ( range ( 1 , 40 ), error , color = red , linestyle = dashed , marker = o , markerfacecolor = blue , markersize = 10 ) plt . title ( Error Rate Nilai K ) plt . xlabel ( Nilai K ) plt . ylabel ( Error rata-rata ) Out [ 31 ]: Text ( 0 , 0.5 , Error rata-rata ) Dari hasil di atas kita dapat melihat bahwa yang memiliki nilai error rata-rata 0 adalah ketika nilai K sebesar 4, 6\u201324. Hasil ini dapat dijadikan pedoman ketika kita ingin menggunakan nilai K yang memiliki akurasi tinggi. Dari situ kita dapat meminimalisir kesalahan prediksi. Kita dapat menampilkan plot hasil klasifikasi data iris. Kemudian kita dapat melihat perubahan yang terjadi ketika nilai K berbeda. Langkah ini sangat membantu jika kita tidak dapat membayangkan bagaimana data tersebut bekerja. In [ 32 ]: import numpy as np import matplotlib.pyplot as plt from matplotlib.colors import ListedColormap from sklearn import neighbors , datasets n_neighbors = 15 # import some data to play with iris = datasets . load_iris () # we only take the first two features. We could avoid this ugly # slicing by using a two-dim dataset X = iris . data [:, : 2 ] y = iris . target h = . 02 # step size in the mesh # Create color maps cmap_light = ListedColormap ([ #FFAAAA , #AAFFAA , #AAAAFF ]) cmap_bold = ListedColormap ([ #FF0000 , #00FF00 , #0000FF ]) for weights in [ uniform , distance ]: # we create an instance of Neighbours Classifier and fit the data. clf = neighbors . KNeighborsClassifier ( n_neighbors , weights = weights ) clf . fit ( X , y ) # Plot the decision boundary. For that, we will assign a color to each # point in the mesh [x_min, x_max]x[y_min, y_max]. x_min , x_max = X [:, 0 ] . min () - 1 , X [:, 0 ] . max () + 1 y_min , y_max = X [:, 1 ] . min () - 1 , X [:, 1 ] . max () + 1 xx , yy = np . meshgrid ( np . arange ( x_min , x_max , h ), np . arange ( y_min , y_max , h )) Z = clf . predict ( np . c_ [ xx . ravel (), yy . ravel ()]) # Put the result into a color plot Z = Z . reshape ( xx . shape ) plt . figure () plt . pcolormesh ( xx , yy , Z , cmap = cmap_light ) # Plot also the training points plt . scatter ( X [:, 0 ], X [:, 1 ], c = y , cmap = cmap_bold , edgecolor = k , s = 20 ) plt . xlim ( xx . min (), xx . max ()) plt . ylim ( yy . min (), yy . max ()) plt . title ( 3-Class classification (k = %i , weights = %s ) % ( n_neighbors , weights )) plt . show () 3-Class classification (k = 15, weights = \u2018uniform\u2019) 3-Class classification (k = 15, weights = \u2018distance\u2019) Kita juga dapat membuat code ketika menjadi dinamis, sehingga memungkinkan adanya interaksi ketika kita memasukkan data. Hanya dengan menambahkan beberapa perintah logika kita dapat melakukan hal tersebut.","title":"K-Nearest Neighbors (KNN)"},{"location":"#k-nearest-neighbors-knn","text":"","title":"K-Nearest Neighbors (KNN)"},{"location":"#definisi","text":"Algoritma K-Nearest Neighbors (KNN) algoritma yang berfungsi untuk melakukan klasifikasi suatu data berdasarkan data pembelajaran ( train data sets ), yang diambil dari k tetangga terdekatnya ( nearest neighbors ). Dengan k merupakan banyaknya tetangga terdekat. KNN ini dapat kita temukan bilamana kita sedang belajar Machine Learning. ML intinya berkaitan dengan automasi atau bagaimana sebuah machine dapat belajar dari contoh-contoh yang kita berikan. Algoritma KNN adalah salah satu yang paling sederhana dari semua algoritma machine learning . Ini hanya menghitung jarak dari titik data baru ke semua titik data pelatihan lainnya. Jarak dapat dari jenis apa pun, misalnya: Euclidean. Kemudian akan memilih titik data K-terdekat, di mana K dapat berupa bilangan bulat apa pun. Kemudian akan diberikan titik data ke kelas tempat mayoritas titik data K berada.","title":"Definisi"},{"location":"#iris-datasets","text":"Kali ini saya menggunakan Python untuk melakukan coba-coba model. Disini saya menggunakan dataset iris. Dataset ini sangat populer digunakan untuk latihan pertama (R atau python). Iris biasanya sudah tersedia didalam modul sklearn (lengkap dengan target dan feature). # code untuk install modul sklearn pip install -U scikit-learn #jika menggunakan pip yang ada di python conda install scikit-learn #jika menggunakan anaconda Terdapat 150 observasi (row) dengan feature/independent variable sebanyak 4 (Panjang sepal, lebar sepal, panjang petal, dan lebar petal). 150 observasi tersebut dibagi menjadi 50 observasi pada masing-masing spesies ( Iris setosa , Iris versicolor , dan Iris virginica ). Pada data iris, kita tidak akan menjumpai nilai null (N/A), sehingga kita tidak perlu capek-capek untuk merapikan data tersebut.","title":"Iris datasets"},{"location":"#knn-dengan-python","text":"Langkah pertama adalah memanggil data iris yang akan kita gunakan untuk membuat KNN. Misal masing-masing target/spesies kita berikan nilai yang unik, setosa=0, versicolor=1, virginica=2. Pada gambar di bawah ini dapat dilihat jika kita menggunakan K=1 dan data baru a=[1,2.7,3.6,4.2]. a mewakili masing-masing nilai feature. Hasilnya adalah 2, yaitu Iris virginica . In [ 3 ]: from sklearn.datasets import load_iris iris = load_iris () x = iris . data y = iris . target from sklearn.neighbors import KNeighborsClassifier import numpy as np knn = KNeighborsClassifier ( n_neighbors = 1 ) #define K=1 knn . fit ( x , y ) a = np . array ([[ 1.0 , 2.7 , 3.6 , 4.2 ]]) knn . predict ( a ) Out [ 3 ]: array ([ 2 ]) Jika saya ganti nilai k=3 dengan nilai data a tidak saya rubah, maka hasil menjadi 1, yaitu Iris versicolor. In [ 8 ]: from sklearn.datasets import load_iris iris = load_iris () x = iris . data y = iris . target from sklearn.neighbors import KNeighborsClassifier import numpy as np knn = KNeighborsClassifier ( n_neighbors = 3 ) #define K=3 knn . fit ( x , y ) a = np . array ([[ 1.0 , 2.7 , 3.6 , 4.2 ]]) knn . predict ( a ) Out [ 8 ]: array ([ 1 ]) Untuk menghindari overfitting, kita dapat membagi iris dataset tadi menjadi data train dan test. Perbandingannya 80%:20%, sehingga bakal ada x buat training dan testing, begitu juga dengan \u2018y\u2019 bakal ada y buat training dan testing. Jadi dari 150 total observasi pada data iris, terdapat 120 observasi pada data train dan 30 observasi pada data testing. Script pemisahan data train dan test dapat dilihat di bawah. from sklearn.model_selection import train_test_split X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.20 )","title":"KNN dengan python"},{"location":"#cross-validation","text":"Cross-validation (CV) adalah metode statistik yang dapat digunakan untuk mengevaluasi kinerja model atau algoritma dimana data dipisahkan menjadi dua subset yaitu data proses pembelajaran dan data validasi / evaluasi. Model atau algoritma dilatih oleh subset pembelajaran dan divalidasi oleh subset validasi. Selanjutnya pemilihan jenis CV dapat didasarkan pada ukuran dataset. Biasanya CV K-fold digunakan karena dapat mengurangi waktu komputasi dengan tetap menjaga keakuratan estimasi. Misal nih, data kita ada 150. Ibarat kita pake K=5, berarti kita bagi 150 data menjadi 5 partisi, isinya masing-masing 30 data. Jangan lupa, kita perlu menentukan mana yang training data dan mana yang test data. Karena perbandingannya 80:20, berarti 120 data adalah training data dan 30 sisanya adalah test data. Berdasarkan 5 partisi tadi, berarti bakal ada 4 partisi x 30 data = 120 training data. Dan sisanya ada 1 partisi test data berisi 30 data. Kemudian, experimen menggunakan data yang udah di partisi-partisi bakal diulang 5 kali (K=5). Tapi posisi partisi Test data berbeda ditiap iterasinya. Misal di iterasi pertama Test nya di posisi partisi awal, terus iterasi partisi kedua Test-nya di posisi kedua, dan seterusnya, pokonya gaboleh sama. In [ 14 ]: import numpy as np import matplotlib.pyplot as plt import pandas as pd In [ 15 ]: url = https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data # Assign colum names to the dataset names = [ sepal-length , sepal-width , petal-length , petal-width , Class ] # Read dataset to pandas dataframe dataset = pd . read_csv ( url , names = names ) In [ 18 ]: X = dataset . iloc [:, : - 1 ] . values y = dataset . iloc [:, 4 ] . values In [ 19 ]: from sklearn.model_selection import train_test_split X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.20 ) In [ 20 ]: from sklearn.preprocessing import StandardScaler scaler = StandardScaler () scaler . fit ( X_train ) X_train = scaler . transform ( X_train ) X_test = scaler . transform ( X_test ) In [ 21 ]: from sklearn.neighbors import KNeighborsClassifier classifier = KNeighborsClassifier ( n_neighbors = 5 ) classifier . fit ( X_train , y_train ) Out [ 21 ]: KNeighborsClassifier ( algorithm = auto , leaf_size = 30 , metric = minkowski , metric_params = None , n_jobs = None , n_neighbors = 5 , p = 2 , weights = uniform ) In [ 23 ]: y_pred = classifier . predict ( X_test ) In [ 25 ]: from sklearn.metrics import classification_report , confusion_matrix print ( confusion_matrix ( y_test , y_pred )) print ( classification_report ( y_test , y_pred )) [[ 11 1 0 ] [ 0 8 0 ] [ 0 0 10 ]] precision recall f1 - score support Iris - setosa 1.00 0.92 0.96 12 Iris - versicolor 0.89 1.00 0.94 8 Iris - virginica 1.00 1.00 1.00 10 micro avg 0.97 0.97 0.97 30 macro avg 0.96 0.97 0.97 30 weighted avg 0.97 0.97 0.97 30 Hasil running code diatas menunjukkan bahwa algoritma KNN yang telah kita lakukan dapat mengklasifikasikan semua observasi (30) dalam set test dengan akurasi 97%, yang artinya susdah sangat bagus. Meskipun algoritma KNN dilakukan sangat baik dengan dataset ini, jangan berharap hasil yang sama dengan semua dataset. KNN tidak selalu berkinerja baik dengan data yang memiliki dimensi tinggi atau yang memiliki feature yang sangat kategoris.","title":"Cross-validation"},{"location":"#menentukan-nilai-k","text":"Nah sekarang kita akan mencari tahu nilai K berapa yang akan menghasilkan akurasi tinggi. Karena kita tahu sendiri pada algoritma KNN, penentuan nilai K sangatlah krusial. Kita akan melihat nilai K dari 1\u201340. Berikut code yang dapat kita gunakan: In [ 14 ]: import numpy as np import matplotlib.pyplot as plt import pandas as pd In [ 15 ]: url = https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data # Assign colum names to the dataset names = [ sepal-length , sepal-width , petal-length , petal-width , Class ] # Read dataset to pandas dataframe dataset = pd . read_csv ( url , names = names ) In [ 18 ]: X = dataset . iloc [:, : - 1 ] . values y = dataset . iloc [:, 4 ] . values In [ 19 ]: from sklearn.model_selection import train_test_split X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.20 ) In [ 20 ]: from sklearn.preprocessing import StandardScaler scaler = StandardScaler () scaler . fit ( X_train ) X_train = scaler . transform ( X_train ) X_test = scaler . transform ( X_test ) In [ 21 ]: from sklearn.neighbors import KNeighborsClassifier classifier = KNeighborsClassifier ( n_neighbors = 5 ) classifier . fit ( X_train , y_train ) Out [ 21 ]: KNeighborsClassifier ( algorithm = auto , leaf_size = 30 , metric = minkowski , metric_params = None , n_jobs = None , n_neighbors = 5 , p = 2 , weights = uniform ) In [ 23 ]: y_pred = classifier . predict ( X_test ) In [ 26 ]: from sklearn.metrics import classification_report , confusion_matrix print ( confusion_matrix ( y_test , y_pred )) print ( classification_report ( y_test , y_pred )) [[ 11 1 0 ] [ 0 8 0 ] [ 0 0 10 ]] precision recall f1 - score support Iris - setosa 1.00 0.92 0.96 12 Iris - versicolor 0.89 1.00 0.94 8 Iris - virginica 1.00 1.00 1.00 10 micro avg 0.97 0.97 0.97 30 macro avg 0.96 0.97 0.97 30 weighted avg 0.97 0.97 0.97 30 In [ 30 ]: error = [] # Calculating error for K values between 1 and 40 for i in range ( 1 , 40 ): knn = KNeighborsClassifier ( n_neighbors = i ) knn . fit ( X_train , y_train ) pred_i = knn . predict ( X_test ) error . append ( np . mean ( pred_i != y_test )) In [ 31 ]: plt . figure ( figsize = ( 12 , 6 )) plt . plot ( range ( 1 , 40 ), error , color = red , linestyle = dashed , marker = o , markerfacecolor = blue , markersize = 10 ) plt . title ( Error Rate Nilai K ) plt . xlabel ( Nilai K ) plt . ylabel ( Error rata-rata ) Out [ 31 ]: Text ( 0 , 0.5 , Error rata-rata ) Dari hasil di atas kita dapat melihat bahwa yang memiliki nilai error rata-rata 0 adalah ketika nilai K sebesar 4, 6\u201324. Hasil ini dapat dijadikan pedoman ketika kita ingin menggunakan nilai K yang memiliki akurasi tinggi. Dari situ kita dapat meminimalisir kesalahan prediksi. Kita dapat menampilkan plot hasil klasifikasi data iris. Kemudian kita dapat melihat perubahan yang terjadi ketika nilai K berbeda. Langkah ini sangat membantu jika kita tidak dapat membayangkan bagaimana data tersebut bekerja. In [ 32 ]: import numpy as np import matplotlib.pyplot as plt from matplotlib.colors import ListedColormap from sklearn import neighbors , datasets n_neighbors = 15 # import some data to play with iris = datasets . load_iris () # we only take the first two features. We could avoid this ugly # slicing by using a two-dim dataset X = iris . data [:, : 2 ] y = iris . target h = . 02 # step size in the mesh # Create color maps cmap_light = ListedColormap ([ #FFAAAA , #AAFFAA , #AAAAFF ]) cmap_bold = ListedColormap ([ #FF0000 , #00FF00 , #0000FF ]) for weights in [ uniform , distance ]: # we create an instance of Neighbours Classifier and fit the data. clf = neighbors . KNeighborsClassifier ( n_neighbors , weights = weights ) clf . fit ( X , y ) # Plot the decision boundary. For that, we will assign a color to each # point in the mesh [x_min, x_max]x[y_min, y_max]. x_min , x_max = X [:, 0 ] . min () - 1 , X [:, 0 ] . max () + 1 y_min , y_max = X [:, 1 ] . min () - 1 , X [:, 1 ] . max () + 1 xx , yy = np . meshgrid ( np . arange ( x_min , x_max , h ), np . arange ( y_min , y_max , h )) Z = clf . predict ( np . c_ [ xx . ravel (), yy . ravel ()]) # Put the result into a color plot Z = Z . reshape ( xx . shape ) plt . figure () plt . pcolormesh ( xx , yy , Z , cmap = cmap_light ) # Plot also the training points plt . scatter ( X [:, 0 ], X [:, 1 ], c = y , cmap = cmap_bold , edgecolor = k , s = 20 ) plt . xlim ( xx . min (), xx . max ()) plt . ylim ( yy . min (), yy . max ()) plt . title ( 3-Class classification (k = %i , weights = %s ) % ( n_neighbors , weights )) plt . show () 3-Class classification (k = 15, weights = \u2018uniform\u2019) 3-Class classification (k = 15, weights = \u2018distance\u2019) Kita juga dapat membuat code ketika menjadi dinamis, sehingga memungkinkan adanya interaksi ketika kita memasukkan data. Hanya dengan menambahkan beberapa perintah logika kita dapat melakukan hal tersebut.","title":"Menentukan nilai K"},{"location":"Data-mining2/","text":"K-Mean Clustering Definisi Clustering merupakan suatu metode untuk mencari dan mengelompokkan data yang memiliki kemiripan karakteristik (similarity) antara satu data dengan data yang lain. Clustering merupakan salah satu metode data mining yang bersifat tanpa arahan (unsupervised). Yang dimaksud metode unsupervised yaitu metode ini diterapkan tanpa adanya latihan (training) dan guru (teacher) serta tidak memerlukan target output. Ada dua jenis data clustering yang sering dipergunakan dalam proses pengelompokan data yaitu hierarchical (hirarki) data clustering dan non-hierarchical (non hirarki) data clustering. K-Means merupakan salah satu metode data clustering non hirarki yang berusaha mempartisi data yang ada ke dalam bentuk satu atau lebih cluster/kelompok. Metode ini mempartisi data ke dalam cluster/kelompok sehingga data yang memiliki karakteristik yang sama dikelompokkan ke dalam satu cluster yang sama dan data yang mempunyai karakteristik yang berbeda dikelompokkan ke dalam kelompok yang lain. Adapun tujuan dari data clustering ini adalah untuk meminimalisasikan objective function yang diset dalam proses clustering, yang pada umumnya berusaha meminimalisasikan variasi di dalam suatu cluster dan memaksimalisasikan variasi antar cluster. Kelebihan K-Means Clustering Menggunakan prinsip yang sederhana, dapat dijelaskan dalam non-statistik Mudah untuk diimplementasikan dan dijalankan. Waktu yang dibutuhkan untuk menjalankan nya relatif cepat Sangat fleksibel, dapat dengan mudah diadaptasi. Sangat umum digunakan Kekurangan K-Means Clustering dalam non-statistik Karena menggunakan k buah acak, tidak di jamin untuk menemukan kumpulan cluster yang optimal dapat terjadinya curse of dimensionality, apabila jarak antara cluster yang satu dengan yang lain memiliki banyak dimesi. Tidak optimal digunakan untuk data yang jumlahnya terlalu banyak sampai bermiliyar. Algoritma K-Means Clustering Tentukan jumlah cluster(kelompok) yang kita inginkan. Inisiasi centroid untuk setiap cluster secara acak. Centroid adalah data yang merepresentasikan suatu kelompok. Hitung kedekatan suatu data terhadap centroid, kemudian masukkan data tersebut ke cluster yang centroid-nya memiliki sifat terdekat dengan dirinya(yang nilainya paling kecil). Pilih kembali centroid untuk masing-masing cluster ,yaitu mean (rata-rata) nilai data dari setiap cluster yang sama . Ulangi langkah ke-3 sampai tidak ada perubahan anggota untuk semua cluster, atau sampai batas yang ditentukan dari perulangan anggota yang sama. Contoh Perhitungan K-Means Sederhana Perhatikan gambar diatas, ada tabel yang mempunyai column X1 dan X2, dimana tabel tersebut akan kita pakai untuk mempelajari k-means clustering. Pertama-tama kita akan menentukan jumlah cluster (kelompok) yakni kita tentukan k = 2. tahap selanjutnya adalah inisiasi centroid karena k = 2 , maka kita akan memilih 2 observasi secara acak dari tabel, yakni A dan C. setelah itu kita akan menghitung jarak antar data dengan centroid yang telah ditentukan, dengan menggunakan formula euclidean distance. Maka pertama-tama kita akan menghitung observasi A dengan centroid 1 : =\u221a(1-1)^2 + (1-1)^2 =\u221a(0)^2 + (0)^2 =\u221a0 =0 Setelah itu menghitung observasi A dengan centroid 2: =\u221a(1-0)^2 + (1-2)^2 =\u221a(1)^2 + (1)^2 =\u221a2 =1,4 Lakukan perhitungan sampai semua observasi dihitung. Hitung kedekatan suatu data terhadap centroid, kemudian masukkan data tersebut ke cluster yang centroid-nya memiliki sifat terdekat dengan dirinya (yang nilainya paling kecil). Pada observasi A yang paling kecil nilainya adalah 0 maka masuk ke dalam cluster 1. Pilih kembali centroid untuk masing-masing cluster ,yaitu mean (rata-rata) nilai data dari setiap cluster yang sama . observasi A dan B dalam cluster yang sama yakni cluster 1, maka dihitung untuk centroid 1 = (1+1 / 2 , 1+ 0 /2 ) - (1,0.5). dan centroid 2 = (1.7,3.7). Lakukan penghitungan lagi dengan centroid yang baru. Pilih kembali centroid untuk masing-masing cluster ,yaitu mean (rata-rata) nilai data dari setiap cluster yang sama. Ulangi sampai tidak ada perubahan anggota untuk semua cluster, atau sampai batas yang ditentukan dari perulangan anggota yang sama.","title":"K-Mean Clustering"},{"location":"Data-mining2/#k-mean-clustering","text":"","title":"K-Mean Clustering"},{"location":"Data-mining2/#definisi","text":"Clustering merupakan suatu metode untuk mencari dan mengelompokkan data yang memiliki kemiripan karakteristik (similarity) antara satu data dengan data yang lain. Clustering merupakan salah satu metode data mining yang bersifat tanpa arahan (unsupervised). Yang dimaksud metode unsupervised yaitu metode ini diterapkan tanpa adanya latihan (training) dan guru (teacher) serta tidak memerlukan target output. Ada dua jenis data clustering yang sering dipergunakan dalam proses pengelompokan data yaitu hierarchical (hirarki) data clustering dan non-hierarchical (non hirarki) data clustering. K-Means merupakan salah satu metode data clustering non hirarki yang berusaha mempartisi data yang ada ke dalam bentuk satu atau lebih cluster/kelompok. Metode ini mempartisi data ke dalam cluster/kelompok sehingga data yang memiliki karakteristik yang sama dikelompokkan ke dalam satu cluster yang sama dan data yang mempunyai karakteristik yang berbeda dikelompokkan ke dalam kelompok yang lain. Adapun tujuan dari data clustering ini adalah untuk meminimalisasikan objective function yang diset dalam proses clustering, yang pada umumnya berusaha meminimalisasikan variasi di dalam suatu cluster dan memaksimalisasikan variasi antar cluster.","title":"Definisi"},{"location":"Data-mining2/#kelebihan-k-means-clustering","text":"Menggunakan prinsip yang sederhana, dapat dijelaskan dalam non-statistik Mudah untuk diimplementasikan dan dijalankan. Waktu yang dibutuhkan untuk menjalankan nya relatif cepat Sangat fleksibel, dapat dengan mudah diadaptasi. Sangat umum digunakan","title":"Kelebihan K-Means Clustering"},{"location":"Data-mining2/#kekurangan-k-means-clustering","text":"dalam non-statistik Karena menggunakan k buah acak, tidak di jamin untuk menemukan kumpulan cluster yang optimal dapat terjadinya curse of dimensionality, apabila jarak antara cluster yang satu dengan yang lain memiliki banyak dimesi. Tidak optimal digunakan untuk data yang jumlahnya terlalu banyak sampai bermiliyar.","title":"Kekurangan K-Means Clustering"},{"location":"Data-mining2/#algoritma-k-means-clustering","text":"Tentukan jumlah cluster(kelompok) yang kita inginkan. Inisiasi centroid untuk setiap cluster secara acak. Centroid adalah data yang merepresentasikan suatu kelompok. Hitung kedekatan suatu data terhadap centroid, kemudian masukkan data tersebut ke cluster yang centroid-nya memiliki sifat terdekat dengan dirinya(yang nilainya paling kecil). Pilih kembali centroid untuk masing-masing cluster ,yaitu mean (rata-rata) nilai data dari setiap cluster yang sama . Ulangi langkah ke-3 sampai tidak ada perubahan anggota untuk semua cluster, atau sampai batas yang ditentukan dari perulangan anggota yang sama. Contoh Perhitungan K-Means Sederhana Perhatikan gambar diatas, ada tabel yang mempunyai column X1 dan X2, dimana tabel tersebut akan kita pakai untuk mempelajari k-means clustering. Pertama-tama kita akan menentukan jumlah cluster (kelompok) yakni kita tentukan k = 2. tahap selanjutnya adalah inisiasi centroid karena k = 2 , maka kita akan memilih 2 observasi secara acak dari tabel, yakni A dan C. setelah itu kita akan menghitung jarak antar data dengan centroid yang telah ditentukan, dengan menggunakan formula euclidean distance. Maka pertama-tama kita akan menghitung observasi A dengan centroid 1 : =\u221a(1-1)^2 + (1-1)^2 =\u221a(0)^2 + (0)^2 =\u221a0 =0 Setelah itu menghitung observasi A dengan centroid 2: =\u221a(1-0)^2 + (1-2)^2 =\u221a(1)^2 + (1)^2 =\u221a2 =1,4 Lakukan perhitungan sampai semua observasi dihitung. Hitung kedekatan suatu data terhadap centroid, kemudian masukkan data tersebut ke cluster yang centroid-nya memiliki sifat terdekat dengan dirinya (yang nilainya paling kecil). Pada observasi A yang paling kecil nilainya adalah 0 maka masuk ke dalam cluster 1. Pilih kembali centroid untuk masing-masing cluster ,yaitu mean (rata-rata) nilai data dari setiap cluster yang sama . observasi A dan B dalam cluster yang sama yakni cluster 1, maka dihitung untuk centroid 1 = (1+1 / 2 , 1+ 0 /2 ) - (1,0.5). dan centroid 2 = (1.7,3.7). Lakukan penghitungan lagi dengan centroid yang baru. Pilih kembali centroid untuk masing-masing cluster ,yaitu mean (rata-rata) nilai data dari setiap cluster yang sama. Ulangi sampai tidak ada perubahan anggota untuk semua cluster, atau sampai batas yang ditentukan dari perulangan anggota yang sama.","title":"Algoritma K-Means Clustering"},{"location":"Data-mining3/","text":"Pohon Keputusan ( Decision Tree ) Definisi Decision tree merupakan suatu metode klasifikasi yang menggunakan struktur pohon, dimana setiap node merepresentasikan atribut dan cabangnya merepresentasikan nilai dari atribut, sedangkan daunnya digunakan untuk merepresentasikan kelas. Node teratas dari decision tree*ini disebut dengan *root . Data dalam pohon keputusan biasanya dinyatakan dalam bentuk tabel dengan atribut dan record. Atribut menyatakan suatu parameter yang dibuat sebagai kriteria dalam pembentukan tree Jenis Algoritma Pohon Keputusan Categorical Variable Decision Tree (Pohon Keputusan Variabel Kategorikal) merupakan pohon keputusan yang memiliki variabel target kategorik Continuous Variable Decision Tree() merupakan Decision Tree yang memiliki variabel target kontinu ## Algoritma Pohon Keputusan ### ID3 Algoritma inti untuk membangun pohon keputusan disebut ID3. Dikembangkan oleh J. R. Quinlan, algoritma ini menggunakan pencarian top-down, melalui ruang cabang yang mungkin tanpa backtracking. ID3 menggunakan Entropy dan Information Gain untuk membuat keputusan Entropy Pohon keputusan dibangun dari atas ke bawah dari simpul akar (root node) dan melibatkan mempartisi data menjadi subset yang berisi instance dengan nilai yang sama (homogen). Algoritma ID3 menggunakan entropi untuk menghitung homogenitas sampel. Jika sampel benar-benar homogen, entropinya nol dan jika sampel dibagi rata maka entropinya satu. Untuk membangun pohon keputusan, kita perlu menghitung dua jenis entropi menggunakan tabel frekuensi sebagai berikut: a. Entropy menggunakan tabel frekuensi satu atribut: b. Entropi menggunakan tabel frekuensi dua atribut: Information Gain Information Gain didasarkan pada penurunan entropi setelah kumpulan data dibagi pada atribut. Membangun pohon keputusan adalah tentang menemukan atribut yang mengembalikan perolehan informasi tertinggi (mis., Cabang yang paling homogen). Langkah 1 : Hitung entropi target. Langkah 2 : Kumpulan data kemudian dibagi pada atribut yang berbeda. Entropi untuk setiap cabang dihitung. Kemudian ditambahkan secara proporsional, untuk mendapatkan total entropi untuk pemisahan. Entropi yang dihasilkan dikurangi dari entropi sebelum pemisahan. Hasilnya adalah Information Gain, atau penurunan entropi. Langkah 3 : Pilih atribut dengan perolehan Information Gain terbesar sebagai simpul keputusan (decision node), bagi dataset dengan cabang-cabangnya dan ulangi proses yang sama pada setiap cabang. Langkah 4-a : Cabang dengan entropi 0 adalah simpul daun. Langkah 4-bb : Cabang dengan entropi lebih dari 0 membutuhkan pemisahan lebih lanjut Langkah 5 : Algoritma ID3 dijalankan secara rekursif pada cabang-cabang non-daun, sampai semua data diklasifikasikan. Gini Index Indeks Gini mengatakan, jika kita memilih dua item dari populasi secara acak maka mereka harus dari kelas yang sama dan probabilitas untuk ini adalah 1 jika populasi murni. Ia bekerja dengan variabel target kategori \"Sukses\" atau \"Kegagalan\". Ini hanya melakukan split Biner Semakin tinggi nilai Gini semakin tinggi homogenitasnya. CART (Klasifikasi dan Pohon Regresi) menggunakan metode Gini untuk membuat pemisahan biner. Rumus Gini Index pi adalah probabilitas bahwa sebuah tuple dalam D milik kelas Ci.Weighted Gini untuk Pemisahan: Langkah-langkah untuk Menghitung Gini untuk pemisahan Hitung Gini untuk sub-node, menggunakan rumus jumlah kuadrat probabilitas untuk keberhasilan dan kegagalan (p\u00b2 + q\u00b2). Hitung Gini untuk split menggunakan skor Gini tertimbang dari setiap node dari split itu Contoh: Mengacu pada contoh di mana kami ingin memisahkan siswa berdasarkan variabel target (playing criket atau tidak). Dalam snapshot di bawah ini, kami membagi populasi menggunakan dua variabel input Gender dan Class. Sekarang, saya ingin mengidentifikasi split mana yang menghasilkan lebih banyak sub-node homogen menggunakan indeks Gini. Pemisahan di gender Gini untuk sub node Female = (0.43)x(0.43)+(0.57)x(0.57)=0.51 Gini untuk sub node Male = (0.56)x(0.56)+(0.44)x(0.44)=0.51 Weighted Gini untuk Pemisahan Gender = (10/30)x0.68+(20/30)x0.55 = 0.59 Pemisahan di class Gini untuk sub node Class IX= (0.2)x(0.2)+(0.8)x(0.8)=0.68 Gini untuk sub node Class X= (0.65)x(0.65)+(0.35)x(0.35)=0.55 Weighted Gini untuk Pemisahan Class = (14/30)x0.51+(16/30)x0.51 = 0.51 ## Kelebihan Kelemahan ### Kelebihan Daerah pengambilan keputusan yang sebelumnya kompleks dan sangat global, dapat diubah menjadi lebih simpel dan spesifik. Eliminasi perhitungan-perhitungan yang tidak diperlukan, karena ketika menggunakan metode decision tree maka sample diuji hanya berdasarkan kriteria atau kelas tertentu. Fleksibel untuk memilih fitur dari internal node yang berbeda, fitur yang terpilih akan membedakan suatu kriteria dibandingkan kriteria yang lain dalam node yang sama. Kefleksibelan metode decision tree ini meningkatkan kualitas keputusan yang dihasilkan jika dibandingkan ketika menggunakan metode penghitungan satu tahap yang lebih konvensional. Dalam analisis multivariat, dengan kriteria dan kelas yang jumlahnya sangat banyak, seorang penguji biasanya perlu untuk mengestimasikan baik itu distribusi dimensi tinggi ataupun parameter tertentu dari distribusi kelas tersebut. Metode decision tree dapat menghindari munculnya permasalahan ini dengan menggunakan criteria yang jumlahnya lebih sedikit pada setiap node internal tanpa banyak mengurangi kualitas keputusan yang dihasilkan. Kekurangan Terjadi overlap terutama ketika kelas-kelas dan criteria yang digunakan jumlahnya sangat banyak. Hal tersebut juga dapat menyebabkan meningkatnya waktu pengambilan keputusan dan jumlah memori yang diperlukan. Pengakumulasian jumlah eror dari setiap tingkat dalam sebuah decision tree yang besar. Kesulitan dalam mendesain decision tree yang optimal. Hasil kualitas keputusan yang didapatkan dari metode decision tree sangat tergantung pada bagaimana pohon tersebut didesain. Implementasi studi kasus kali ini, saya akan menggunakan dataset user modeling. Import Library import pandas as pd from sklearn.tree import DecisionTreeClassifier from sklearn import model_selection from sklearn import metrics from sklearn.model_selection import train_test_split from sklearn.tree import export_graphviz from sklearn.externals.six import StringIO import pydotplus from IPython.display import Image Mengimport dataset yang akan di buat menjadi pohon keputusan #memuat file excel df = pd . read_csv ( Iris.csv ) #menampilkan data print ( Informasi Data \\n ) print ( Jumlah Data : , len ( df )) print ( Dimensi Data : , df . shape ) print ( Dataset : ) print ( df . head ()) print ( \\n ) Output : Menampilkan 5 Data teratas Menampilkan jumlah masing-masing diagnosis dan kolom diagnosis digunakan sebagai class nantinya. #10 baris pertama df . head ( 10 ) Output : #splitting dataset ke training dan testing train , test = train_test_split ( df , test_size = 0.1 , random_state = 1234 ) #mencari hasil print ( train . shape ) print ( test . shape ) Output : # Dataset validasi dataset array = df . values X = array [:, 1 : 5 ] Y = array [:, 5 ] # Sepertiga data sebagai bagian dari set tes validation_size = 15 seed = 7 X_train , X_validation , Y_train , Y_validation = model_selection . train_test_split ( X , Y , test_size = validation_size , random_state = seed ) #mencari hasil print ( X_train . shape ) print ( Y_train . shape ) print ( X_validation . shape ) print ( Y_validation . shape ) Output : entropy = DecisionTreeClassifier ( criterion = entropy , random_state = 1234 ) #learning entropy . fit ( X_train , Y_train ) #Prediksi prediction = entropy . predict ( X_validation ) #mengevaluasi(Accuracy) print ( Accuracy: , metrics . accuracy_score ( prediction , Y_validation )) #evaluation(Confusion Metrix) print ( Confusion Metrix: \\n , metrics . confusion_matrix ( prediction , Y_validation )) Output : Visualisasi Decision Tree feature_cols = [ SepalLengthCm , SepalWidthCm , PetalLengthCm , PetalWidthCm ] dot_data = StringIO () export_graphviz ( entropy , out_file = dot_data , filled = True , rounded = True , special_characters = True , feature_names = feature_cols , class_names = [ Iris-setosa , Iris-versicolor , Iris-virginica ]) graph = pydotplus . graph_from_dot_data ( dot_data . getvalue ()) graph . write_png ( entropy.png ) Image ( graph . create_png ()) Output : print ( Hasil prediksi menngunakan entropy ) #Membentuk kembali diperlukan untuk melakukan penggabungan pred_clf_df = pd . DataFrame ( prediction . reshape ( 15 , 1 )) #Ganti nama kolom untuk menunjukkan prediksi pred_clf_df . rename ( columns = { 0 : Prediction }, inplace = True ) #membentuk kembali dataset uji X_validation_df = pd . DataFrame ( X_validation . reshape ( 15 , 4 )) #menggabungkan dua bingkai data panda di atas kolom untuk membuat dataset prediksi pred_outcome = pd . concat ([ X_validation_df , pred_clf_df ], axis = 1 , join_axes = [ X_validation_df . index ]) pred_outcome . rename ( columns = { 0 : SepalLengthCm , 1 : SepalWidthCm , 2 : Number_of_Warts , 3 : PetalLengthCm , 4 : Area }, inplace = True ) #cetak 10 baris prediksi akhir print (( pred_outcome ) . head ( 15 )) print ( \\n ) #mengevaluasi(Accuracy) print ( Accuracy: , metrics . accuracy_score ( prediction , Y_validation )) Output : gini = DecisionTreeClassifier ( criterion = gini , random_state = 1234 ) #learning gini . fit ( X_train , Y_train ) #Prediksi prediction_gini = gini . predict ( X_validation ) #mengevaluasi(Accuracy) print ( Accuracy: , metrics . accuracy_score ( prediction_gini , Y_validation )) #evaluation(Confusion Metrix) print ( Confusion Metrix: \\n , metrics . confusion_matrix ( prediction_gini , Y_validation )) Output : Penederhanaan Decision Tree feature_cols = [ SepalLengthCm , SepalWidthCm , PetalLengthCm , PetalWidthCm ] dot_data = StringIO () export_graphviz ( gini , out_file = dot_data , filled = True , rounded = True , special_characters = True , feature_names = feature_cols , class_names = [ Iris-setosa , Iris-versicolor , Iris-virginica ]) graph = pydotplus . graph_from_dot_data ( dot_data . getvalue ()) graph . write_png ( gini.png ) Image ( graph . create_png ()) Output : print ( Hasil prediksi menngunakan gini ) #Membentuk kembali diperlukan untuk melakukan penggabungan pred_clf_df = pd . DataFrame ( prediction . reshape ( 15 , 1 )) #Ganti nama kolom untuk menunjukkan prediksi pred_clf_df . rename ( columns = { 0 : Prediction }, inplace = True ) #membentuk kembali dataset uji X_validation_df = pd . DataFrame ( X_validation . reshape ( 15 , 4 )) #menggabungkan dua bingkai data panda di atas kolom untuk membuat dataset prediksi pred_outcome = pd . concat ([ X_validation_df , pred_clf_df ], axis = 1 , join_axes = [ X_validation_df . index ]) pred_outcome . rename ( columns = { 0 : age , 1 : Time , 2 : Number_of_Warts , 3 : Type , 4 : Area }, inplace = True ) #cetak 10 baris prediksi akhir print (( pred_outcome ) . head ( 15 )) print ( \\n ) #mengevaluasi(Accuracy) print ( Accuracy: , metrics . accuracy_score ( prediction_gini , Y_validation )) Output :","title":"Pohon Keputusan (Decision Tree)"},{"location":"Data-mining3/#pohon-keputusan-decision-tree","text":"","title":"Pohon Keputusan (Decision Tree)"},{"location":"Data-mining3/#definisi","text":"Decision tree merupakan suatu metode klasifikasi yang menggunakan struktur pohon, dimana setiap node merepresentasikan atribut dan cabangnya merepresentasikan nilai dari atribut, sedangkan daunnya digunakan untuk merepresentasikan kelas. Node teratas dari decision tree*ini disebut dengan *root . Data dalam pohon keputusan biasanya dinyatakan dalam bentuk tabel dengan atribut dan record. Atribut menyatakan suatu parameter yang dibuat sebagai kriteria dalam pembentukan tree","title":"Definisi"},{"location":"Data-mining3/#jenis-algoritma-pohon-keputusan","text":"Categorical Variable Decision Tree (Pohon Keputusan Variabel Kategorikal) merupakan pohon keputusan yang memiliki variabel target kategorik Continuous Variable Decision Tree() merupakan Decision Tree yang memiliki variabel target kontinu ## Algoritma Pohon Keputusan ### ID3 Algoritma inti untuk membangun pohon keputusan disebut ID3. Dikembangkan oleh J. R. Quinlan, algoritma ini menggunakan pencarian top-down, melalui ruang cabang yang mungkin tanpa backtracking. ID3 menggunakan Entropy dan Information Gain untuk membuat keputusan","title":"Jenis Algoritma Pohon Keputusan"},{"location":"Data-mining3/#entropy","text":"Pohon keputusan dibangun dari atas ke bawah dari simpul akar (root node) dan melibatkan mempartisi data menjadi subset yang berisi instance dengan nilai yang sama (homogen). Algoritma ID3 menggunakan entropi untuk menghitung homogenitas sampel. Jika sampel benar-benar homogen, entropinya nol dan jika sampel dibagi rata maka entropinya satu. Untuk membangun pohon keputusan, kita perlu menghitung dua jenis entropi menggunakan tabel frekuensi sebagai berikut: a. Entropy menggunakan tabel frekuensi satu atribut: b. Entropi menggunakan tabel frekuensi dua atribut:","title":"Entropy"},{"location":"Data-mining3/#information-gain","text":"Information Gain didasarkan pada penurunan entropi setelah kumpulan data dibagi pada atribut. Membangun pohon keputusan adalah tentang menemukan atribut yang mengembalikan perolehan informasi tertinggi (mis., Cabang yang paling homogen). Langkah 1 : Hitung entropi target. Langkah 2 : Kumpulan data kemudian dibagi pada atribut yang berbeda. Entropi untuk setiap cabang dihitung. Kemudian ditambahkan secara proporsional, untuk mendapatkan total entropi untuk pemisahan. Entropi yang dihasilkan dikurangi dari entropi sebelum pemisahan. Hasilnya adalah Information Gain, atau penurunan entropi. Langkah 3 : Pilih atribut dengan perolehan Information Gain terbesar sebagai simpul keputusan (decision node), bagi dataset dengan cabang-cabangnya dan ulangi proses yang sama pada setiap cabang. Langkah 4-a : Cabang dengan entropi 0 adalah simpul daun. Langkah 4-bb : Cabang dengan entropi lebih dari 0 membutuhkan pemisahan lebih lanjut Langkah 5 : Algoritma ID3 dijalankan secara rekursif pada cabang-cabang non-daun, sampai semua data diklasifikasikan.","title":"Information Gain"},{"location":"Data-mining3/#gini-index","text":"Indeks Gini mengatakan, jika kita memilih dua item dari populasi secara acak maka mereka harus dari kelas yang sama dan probabilitas untuk ini adalah 1 jika populasi murni. Ia bekerja dengan variabel target kategori \"Sukses\" atau \"Kegagalan\". Ini hanya melakukan split Biner Semakin tinggi nilai Gini semakin tinggi homogenitasnya. CART (Klasifikasi dan Pohon Regresi) menggunakan metode Gini untuk membuat pemisahan biner.","title":"Gini Index"},{"location":"Data-mining3/#rumus-gini-index","text":"pi adalah probabilitas bahwa sebuah tuple dalam D milik kelas Ci.Weighted Gini untuk Pemisahan:","title":"Rumus Gini Index"},{"location":"Data-mining3/#langkah-langkah-untuk-menghitung-gini-untuk-pemisahan","text":"Hitung Gini untuk sub-node, menggunakan rumus jumlah kuadrat probabilitas untuk keberhasilan dan kegagalan (p\u00b2 + q\u00b2). Hitung Gini untuk split menggunakan skor Gini tertimbang dari setiap node dari split itu Contoh: Mengacu pada contoh di mana kami ingin memisahkan siswa berdasarkan variabel target (playing criket atau tidak). Dalam snapshot di bawah ini, kami membagi populasi menggunakan dua variabel input Gender dan Class. Sekarang, saya ingin mengidentifikasi split mana yang menghasilkan lebih banyak sub-node homogen menggunakan indeks Gini. Pemisahan di gender Gini untuk sub node Female = (0.43)x(0.43)+(0.57)x(0.57)=0.51 Gini untuk sub node Male = (0.56)x(0.56)+(0.44)x(0.44)=0.51 Weighted Gini untuk Pemisahan Gender = (10/30)x0.68+(20/30)x0.55 = 0.59 Pemisahan di class Gini untuk sub node Class IX= (0.2)x(0.2)+(0.8)x(0.8)=0.68 Gini untuk sub node Class X= (0.65)x(0.65)+(0.35)x(0.35)=0.55 Weighted Gini untuk Pemisahan Class = (14/30)x0.51+(16/30)x0.51 = 0.51 ## Kelebihan Kelemahan ### Kelebihan Daerah pengambilan keputusan yang sebelumnya kompleks dan sangat global, dapat diubah menjadi lebih simpel dan spesifik. Eliminasi perhitungan-perhitungan yang tidak diperlukan, karena ketika menggunakan metode decision tree maka sample diuji hanya berdasarkan kriteria atau kelas tertentu. Fleksibel untuk memilih fitur dari internal node yang berbeda, fitur yang terpilih akan membedakan suatu kriteria dibandingkan kriteria yang lain dalam node yang sama. Kefleksibelan metode decision tree ini meningkatkan kualitas keputusan yang dihasilkan jika dibandingkan ketika menggunakan metode penghitungan satu tahap yang lebih konvensional. Dalam analisis multivariat, dengan kriteria dan kelas yang jumlahnya sangat banyak, seorang penguji biasanya perlu untuk mengestimasikan baik itu distribusi dimensi tinggi ataupun parameter tertentu dari distribusi kelas tersebut. Metode decision tree dapat menghindari munculnya permasalahan ini dengan menggunakan criteria yang jumlahnya lebih sedikit pada setiap node internal tanpa banyak mengurangi kualitas keputusan yang dihasilkan.","title":"Langkah-langkah untuk Menghitung Gini untuk pemisahan"},{"location":"Data-mining3/#kekurangan","text":"Terjadi overlap terutama ketika kelas-kelas dan criteria yang digunakan jumlahnya sangat banyak. Hal tersebut juga dapat menyebabkan meningkatnya waktu pengambilan keputusan dan jumlah memori yang diperlukan. Pengakumulasian jumlah eror dari setiap tingkat dalam sebuah decision tree yang besar. Kesulitan dalam mendesain decision tree yang optimal. Hasil kualitas keputusan yang didapatkan dari metode decision tree sangat tergantung pada bagaimana pohon tersebut didesain.","title":"Kekurangan"},{"location":"Data-mining3/#implementasi","text":"studi kasus kali ini, saya akan menggunakan dataset user modeling. Import Library import pandas as pd from sklearn.tree import DecisionTreeClassifier from sklearn import model_selection from sklearn import metrics from sklearn.model_selection import train_test_split from sklearn.tree import export_graphviz from sklearn.externals.six import StringIO import pydotplus from IPython.display import Image Mengimport dataset yang akan di buat menjadi pohon keputusan #memuat file excel df = pd . read_csv ( Iris.csv ) #menampilkan data print ( Informasi Data \\n ) print ( Jumlah Data : , len ( df )) print ( Dimensi Data : , df . shape ) print ( Dataset : ) print ( df . head ()) print ( \\n ) Output : Menampilkan 5 Data teratas Menampilkan jumlah masing-masing diagnosis dan kolom diagnosis digunakan sebagai class nantinya. #10 baris pertama df . head ( 10 ) Output : #splitting dataset ke training dan testing train , test = train_test_split ( df , test_size = 0.1 , random_state = 1234 ) #mencari hasil print ( train . shape ) print ( test . shape ) Output : # Dataset validasi dataset array = df . values X = array [:, 1 : 5 ] Y = array [:, 5 ] # Sepertiga data sebagai bagian dari set tes validation_size = 15 seed = 7 X_train , X_validation , Y_train , Y_validation = model_selection . train_test_split ( X , Y , test_size = validation_size , random_state = seed ) #mencari hasil print ( X_train . shape ) print ( Y_train . shape ) print ( X_validation . shape ) print ( Y_validation . shape ) Output : entropy = DecisionTreeClassifier ( criterion = entropy , random_state = 1234 ) #learning entropy . fit ( X_train , Y_train ) #Prediksi prediction = entropy . predict ( X_validation ) #mengevaluasi(Accuracy) print ( Accuracy: , metrics . accuracy_score ( prediction , Y_validation )) #evaluation(Confusion Metrix) print ( Confusion Metrix: \\n , metrics . confusion_matrix ( prediction , Y_validation )) Output : Visualisasi Decision Tree feature_cols = [ SepalLengthCm , SepalWidthCm , PetalLengthCm , PetalWidthCm ] dot_data = StringIO () export_graphviz ( entropy , out_file = dot_data , filled = True , rounded = True , special_characters = True , feature_names = feature_cols , class_names = [ Iris-setosa , Iris-versicolor , Iris-virginica ]) graph = pydotplus . graph_from_dot_data ( dot_data . getvalue ()) graph . write_png ( entropy.png ) Image ( graph . create_png ()) Output : print ( Hasil prediksi menngunakan entropy ) #Membentuk kembali diperlukan untuk melakukan penggabungan pred_clf_df = pd . DataFrame ( prediction . reshape ( 15 , 1 )) #Ganti nama kolom untuk menunjukkan prediksi pred_clf_df . rename ( columns = { 0 : Prediction }, inplace = True ) #membentuk kembali dataset uji X_validation_df = pd . DataFrame ( X_validation . reshape ( 15 , 4 )) #menggabungkan dua bingkai data panda di atas kolom untuk membuat dataset prediksi pred_outcome = pd . concat ([ X_validation_df , pred_clf_df ], axis = 1 , join_axes = [ X_validation_df . index ]) pred_outcome . rename ( columns = { 0 : SepalLengthCm , 1 : SepalWidthCm , 2 : Number_of_Warts , 3 : PetalLengthCm , 4 : Area }, inplace = True ) #cetak 10 baris prediksi akhir print (( pred_outcome ) . head ( 15 )) print ( \\n ) #mengevaluasi(Accuracy) print ( Accuracy: , metrics . accuracy_score ( prediction , Y_validation )) Output : gini = DecisionTreeClassifier ( criterion = gini , random_state = 1234 ) #learning gini . fit ( X_train , Y_train ) #Prediksi prediction_gini = gini . predict ( X_validation ) #mengevaluasi(Accuracy) print ( Accuracy: , metrics . accuracy_score ( prediction_gini , Y_validation )) #evaluation(Confusion Metrix) print ( Confusion Metrix: \\n , metrics . confusion_matrix ( prediction_gini , Y_validation )) Output : Penederhanaan Decision Tree feature_cols = [ SepalLengthCm , SepalWidthCm , PetalLengthCm , PetalWidthCm ] dot_data = StringIO () export_graphviz ( gini , out_file = dot_data , filled = True , rounded = True , special_characters = True , feature_names = feature_cols , class_names = [ Iris-setosa , Iris-versicolor , Iris-virginica ]) graph = pydotplus . graph_from_dot_data ( dot_data . getvalue ()) graph . write_png ( gini.png ) Image ( graph . create_png ()) Output : print ( Hasil prediksi menngunakan gini ) #Membentuk kembali diperlukan untuk melakukan penggabungan pred_clf_df = pd . DataFrame ( prediction . reshape ( 15 , 1 )) #Ganti nama kolom untuk menunjukkan prediksi pred_clf_df . rename ( columns = { 0 : Prediction }, inplace = True ) #membentuk kembali dataset uji X_validation_df = pd . DataFrame ( X_validation . reshape ( 15 , 4 )) #menggabungkan dua bingkai data panda di atas kolom untuk membuat dataset prediksi pred_outcome = pd . concat ([ X_validation_df , pred_clf_df ], axis = 1 , join_axes = [ X_validation_df . index ]) pred_outcome . rename ( columns = { 0 : age , 1 : Time , 2 : Number_of_Warts , 3 : Type , 4 : Area }, inplace = True ) #cetak 10 baris prediksi akhir print (( pred_outcome ) . head ( 15 )) print ( \\n ) #mengevaluasi(Accuracy) print ( Accuracy: , metrics . accuracy_score ( prediction_gini , Y_validation )) Output :","title":"Implementasi"},{"location":"extensions/admonition/","text":"Admonition Admonition is an extension included in the standard Markdown library that makes it possible to add block-styled side content to your documentation, for example summaries, notes, hints or warnings. Installation Add the following lines to your mkdocs.yml : markdown_extensions : - admonition Usage Admonition blocks follow a simple syntax: every block is started with !!! , followed by a single keyword which is used as the type qualifier of the block. The content of the block then follows on the next line, indented by four spaces. Example: !!! note Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Result: Note Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Changing the title By default, the block title will equal the type qualifier in titlecase. However, it can easily be changed by adding a quoted string after the type qualifier. Example: !!! note Phasellus posuere in sem ut cursus Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Result: Phasellus posuere in sem ut cursus Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Removing the title Similar to setting a custom title , the icon and title can be omitted by providing an empty string after the type qualifier: Example: !!! note Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Result: Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Embedded code blocks Blocks can contain all kinds of text content, including headlines, lists, paragraphs and other blocks \u2013 except code blocks, because the parser from the standard Markdown library does not account for those. However, the PyMdown Extensions package adds an extension called SuperFences , which makes it possible to nest code blocks within other blocks, respectively Admonition blocks. Example: Note Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. SELECT Employees . EmployeeID , Employees . Name , Employees . Salary , Manager . Name AS Manager FROM Employees LEFT JOIN Employees AS Manager ON Employees . ManagerID = Manager . EmployeeID WHERE Employees . EmployeeID = 087652 ; Nunc eu odio eleifend, blandit leo a, volutpat sapien. Phasellus posuere in sem ut cursus. Nullam sit amet tincidunt ipsum, sit amet elementum turpis. Etiam ipsum quam, mattis in purus vitae, lacinia fermentum enim. Collapsible blocks The Details extension which is also part of the PyMdown Extensions package adds support for rendering collapsible Admonition blocks. This is useful for FAQs or content that is of secondary nature. Example: ??? note Phasellus posuere in sem ut cursus Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Result: Phasellus posuere in sem ut cursus Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. By adding a + sign directly after the start marker, blocks can be rendered open by default. Types Admonition supports user-defined type qualifiers which may influence the style of the inserted block. Following is a list of type qualifiers provided by the Material theme, whereas the default type, and thus fallback for unknown type qualifiers, is note . Note Example: !!! note Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Result: Note Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Qualifiers: note seealso Abstract Example: !!! abstract Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Result: Abstract Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Qualifiers: abstract summary tldr Info Example: !!! info Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Result: Info Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Qualifiers: info todo Tip Example: !!! tip Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Result: Tip Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Qualifiers: tip hint important Success Example: !!! success Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Result: Success Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Qualifiers: success check done Question Example: !!! question Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Result: Question Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Qualifiers: question help faq Warning Example: !!! warning Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Result: Warning Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Qualifiers: warning caution attention Failure Example: !!! failure Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Result: Failure Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Qualifiers: failure fail missing Danger Example: !!! danger Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Result: Danger Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Qualifiers: danger error Bug Example: !!! bug Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Result: Bug Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Qualifiers: bug Example Example: !!! example Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Result: Example Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Qualifiers: example snippet Quote Example: !!! quote Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Result: Quote Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Qualifiers: quote cite","title":"Admonition"},{"location":"extensions/admonition/#admonition","text":"Admonition is an extension included in the standard Markdown library that makes it possible to add block-styled side content to your documentation, for example summaries, notes, hints or warnings.","title":"Admonition"},{"location":"extensions/admonition/#installation","text":"Add the following lines to your mkdocs.yml : markdown_extensions : - admonition","title":"Installation"},{"location":"extensions/admonition/#usage","text":"Admonition blocks follow a simple syntax: every block is started with !!! , followed by a single keyword which is used as the type qualifier of the block. The content of the block then follows on the next line, indented by four spaces. Example: !!! note Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Result: Note Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.","title":"Usage"},{"location":"extensions/admonition/#changing-the-title","text":"By default, the block title will equal the type qualifier in titlecase. However, it can easily be changed by adding a quoted string after the type qualifier. Example: !!! note Phasellus posuere in sem ut cursus Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Result: Phasellus posuere in sem ut cursus Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.","title":"Changing the title"},{"location":"extensions/admonition/#removing-the-title","text":"Similar to setting a custom title , the icon and title can be omitted by providing an empty string after the type qualifier: Example: !!! note Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Result: Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.","title":"Removing the title"},{"location":"extensions/admonition/#embedded-code-blocks","text":"Blocks can contain all kinds of text content, including headlines, lists, paragraphs and other blocks \u2013 except code blocks, because the parser from the standard Markdown library does not account for those. However, the PyMdown Extensions package adds an extension called SuperFences , which makes it possible to nest code blocks within other blocks, respectively Admonition blocks. Example: Note Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. SELECT Employees . EmployeeID , Employees . Name , Employees . Salary , Manager . Name AS Manager FROM Employees LEFT JOIN Employees AS Manager ON Employees . ManagerID = Manager . EmployeeID WHERE Employees . EmployeeID = 087652 ; Nunc eu odio eleifend, blandit leo a, volutpat sapien. Phasellus posuere in sem ut cursus. Nullam sit amet tincidunt ipsum, sit amet elementum turpis. Etiam ipsum quam, mattis in purus vitae, lacinia fermentum enim.","title":"Embedded code blocks"},{"location":"extensions/admonition/#collapsible-blocks","text":"The Details extension which is also part of the PyMdown Extensions package adds support for rendering collapsible Admonition blocks. This is useful for FAQs or content that is of secondary nature. Example: ??? note Phasellus posuere in sem ut cursus Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Result: Phasellus posuere in sem ut cursus Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. By adding a + sign directly after the start marker, blocks can be rendered open by default.","title":"Collapsible blocks"},{"location":"extensions/admonition/#types","text":"Admonition supports user-defined type qualifiers which may influence the style of the inserted block. Following is a list of type qualifiers provided by the Material theme, whereas the default type, and thus fallback for unknown type qualifiers, is note .","title":"Types"},{"location":"extensions/admonition/#note","text":"Example: !!! note Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Result: Note Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Qualifiers: note seealso","title":"Note"},{"location":"extensions/admonition/#abstract","text":"Example: !!! abstract Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Result: Abstract Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Qualifiers: abstract summary tldr","title":"Abstract"},{"location":"extensions/admonition/#info","text":"Example: !!! info Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Result: Info Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Qualifiers: info todo","title":"Info"},{"location":"extensions/admonition/#tip","text":"Example: !!! tip Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Result: Tip Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Qualifiers: tip hint important","title":"Tip"},{"location":"extensions/admonition/#success","text":"Example: !!! success Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Result: Success Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Qualifiers: success check done","title":"Success"},{"location":"extensions/admonition/#question","text":"Example: !!! question Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Result: Question Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Qualifiers: question help faq","title":"Question"},{"location":"extensions/admonition/#warning","text":"Example: !!! warning Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Result: Warning Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Qualifiers: warning caution attention","title":"Warning"},{"location":"extensions/admonition/#failure","text":"Example: !!! failure Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Result: Failure Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Qualifiers: failure fail missing","title":"Failure"},{"location":"extensions/admonition/#danger","text":"Example: !!! danger Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Result: Danger Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Qualifiers: danger error","title":"Danger"},{"location":"extensions/admonition/#bug","text":"Example: !!! bug Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Result: Bug Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Qualifiers: bug","title":"Bug"},{"location":"extensions/admonition/#example","text":"Example: !!! example Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Result: Example Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Qualifiers: example snippet","title":"Example"},{"location":"extensions/admonition/#quote","text":"Example: !!! quote Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Result: Quote Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Qualifiers: quote cite","title":"Quote"},{"location":"extensions/codehilite/","text":"CodeHilite CodeHilite is an extension that adds syntax highlighting to code blocks and is included in the standard Markdown library. The highlighting process is executed during compilation of the Markdown file. Syntax highlighting not working? Please ensure that Pygments is installed. See the next section for further directions on how to set up Pygments or use the official Docker image with all dependencies pre-installed. Installation CodeHilite parses code blocks and wraps them in pre tags. If Pygments is installed, which is a generic syntax highlighter with support for over 300 languages , CodeHilite will also highlight the code block. Pygments can be installed with the following command: pip install pygments To enable CodeHilite, add the following lines to your mkdocs.yml : markdown_extensions : - codehilite Usage Specifying the language The CodeHilite extension uses the same syntax as regular Markdown code blocks, but needs to know the language of the code block. This can be done in three different ways. via Markdown syntax recommended In Markdown, code blocks can be opened and closed by writing three backticks on separate lines. To add code highlighting to those blocks, the easiest way is to specify the language directly after the opening block. Example: ``` python import tensorflow as tf ``` Result: import tensorflow as tf via Shebang Alternatively, if the first line of a code block contains a shebang, the language is derived from the path referenced in the shebang. This will only work for code blocks that are indented using four spaces, not for those encapsulated in three backticks. Example: #!/usr/bin/python import tensorflow as tf Result: #!/usr/bin/python import tensorflow as tf via three colons If the first line starts with three colons followed by a language identifier, the first line is stripped. This will only work for code blocks that are indented using four spaces, not for those encapsulated in three backticks. Example: :::python import tensorflow as tf Result: import tensorflow as tf Adding line numbers Line numbers can be added by enabling the linenums flag in your mkdocs.yml : markdown_extensions : - codehilite : linenums : true Example: ``` python Bubble sort def bubble_sort(items): for i in range(len(items)): for j in range(len(items) - 1 - i): if items[j] items[j + 1]: items[j], items[j + 1] = items[j + 1], items[j] ``` Result: 1 2 3 4 5 6 Bubble sort def bubble_sort ( items ): for i in range ( len ( items )): for j in range ( len ( items ) - 1 - i ): if items [ j ] items [ j + 1 ]: items [ j ], items [ j + 1 ] = items [ j + 1 ], items [ j ] Grouping code blocks The SuperFences extension which is part of the PyMdown Extensions package adds support for grouping code blocks with tabs. This is especially useful for documenting projects with multiple language bindings. Example: ``` bash tab= Bash #!/bin/bash echo Hello world! ``` ``` c tab= C #include stdio.h int main(void) { printf( Hello world!\\n ); } ``` ``` c++ tab= C++ #include iostream int main() { std::cout Hello world! std::endl; return 0; } ``` ``` c# tab= C# using System; class Program { static void Main(string[] args) { Console.WriteLine( Hello world! ); } } ``` Result: Bash #!/bin/bash echo Hello world! C #include stdio.h int main ( void ) { printf ( Hello world! \\n ); } C++ #include iostream int main () { std :: cout Hello world! std :: endl ; return 0 ; } C# using System ; class Program { static void Main ( string [] args ) { Console . WriteLine ( Hello world! ); } } Highlighting specific lines Specific lines can be highlighted by passing the line numbers to the hl_lines argument placed right after the language identifier. Line counts start at 1. Example: ``` python hl_lines= 3 4 Bubble sort def bubble_sort(items): for i in range(len(items)): for j in range(len(items) - 1 - i): if items[j] items[j + 1]: items[j], items[j + 1] = items[j + 1], items[j] ``` Result: 1 2 3 4 5 6 Bubble sort def bubble_sort ( items ): for i in range ( len ( items )): for j in range ( len ( items ) - 1 - i ): if items [ j ] items [ j + 1 ]: items [ j ], items [ j + 1 ] = items [ j + 1 ], items [ j ] Supported languages excerpt CodeHilite uses Pygments , a generic syntax highlighter with support for over 300 languages , so the following list of examples is just an excerpt. Bash #!/bin/bash for OPT in $@ do case $OPT in -f ) canonicalize = 1 ;; -n ) switchlf = -n ;; esac done # readlink -f function __readlink_f { target = $1 while test -n $target ; do filepath = $target cd ` dirname $filepath ` target = ` readlink $filepath ` done /bin/echo $switchlf ` pwd -P ` / ` basename $filepath ` } if [ ! $canonicalize ] ; then readlink $switchlf $@ else for file in $@ do case $file in -* ) ;; * ) __readlink_f $file ;; esac done fi exit $? C extern size_t pb_varint_scan ( const uint8_t data [], size_t left ) { assert ( data left ); left = left 10 ? 10 : left ; #ifdef __SSE2__ /* Mapping: remaining bytes == bitmask */ static const int mask_map [] = { 0x0000 , 0x0001 , 0x0003 , 0x0007 , 0x000F , 0x001F , 0x003F , 0x007F , 0x00FF , 0x01FF , 0x03FF }; /* Load buffer into 128-bit integer and create high-bit mask */ __m128i temp = _mm_loadu_si128 (( const __m128i * ) data ); __m128i high = _mm_set1_epi8 ( 0x80 ); /* Intersect and extract mask with high-bits set */ int mask = _mm_movemask_epi8 ( _mm_and_si128 ( temp , high )); mask = ( mask mask_map [ left ]) ^ mask_map [ left ]; /* Count trailing zeroes */ return mask ? __builtin_ctz ( mask ) + 1 : 0 ; #else /* Linear scan */ size_t size = 0 ; while ( data [ size ++ ] 0x80 ) if ( !-- left ) return 0 ; return size ; #endif /* __SSE2__ */ } C++ Extension :: Extension ( const Descriptor * descriptor , const Descriptor * scope ) : descriptor_ ( descriptor ), scope_ ( scope ) { /* Extract full name for signature */ variables_ [ signature ] = descriptor_ - full_name (); /* Prepare message symbol */ variables_ [ message ] = StringReplace ( variables_ [ signature ], . , _ , true ); LowerString ( ( variables_ [ message ])); /* Suffix scope to identifiers, if given */ string suffix ( ); if ( scope_ ) { suffix = scope_ - full_name (); /* Check if the base and extension types are in the same package */ if ( ! scope_ - file () - package (). compare ( descriptor_ - file () - package ())) suffix = StripPrefixString ( suffix , scope_ - file () - package () + . ); /* Append to signature */ variables_ [ signature ] += .[ + suffix + ] ; suffix = _ + suffix ; } /* Prepare extension symbol */ variables_ [ extension ] = StringReplace ( suffix , . , _ , true ); LowerString ( ( variables_ [ extension ])); } C #35 ; public static void Send ( Socket socket , byte [] buffer , int offset , int size , int timeout ) { int startTickCount = Environment . TickCount ; int sent = 0 ; do { if ( Environment . TickCount startTickCount + timeout ) throw new Exception ( Timeout. ); try { sent += socket . Send ( buffer , offset + sent , size - sent , SocketFlags . None ); } catch ( SocketException ex ) { if ( ex . SocketErrorCode == SocketError . WouldBlock || ex . SocketErrorCode == SocketError . IOPending || ex . SocketErrorCode == SocketError . NoBufferSpaceAvailable ) { /* Socket buffer is probably full, wait and try again */ Thread . Sleep ( 30 ); } else { throw ex ; } } } while ( sent size ); } Clojure ( clojure-version ) ( defn partition-when [ f ] ( fn [ rf ] ( let [ a ( java.util.ArrayList. ) fval ( volatile! false )] ( fn ([] ( rf )) ([ result ] ( let [ result ( if ( .isEmpty a ) result ( let [ v ( vec ( .toArray a ))] ;; Clear first ( .clear a ) ( unreduced ( rf result v ))))] ( rf result ))) ([ result input ] ( if-not ( and ( f input ) @ fval ) ( do ( vreset! fval true ) ( .add a input ) result ) ( let [ v ( vec ( .toArray a ))] ( .clear a ) ( let [ ret ( rf result v )] ( when-not ( reduced? ret ) ( .add a input )) ret )))))))) ( into [] ( partition-when # ( .startsWith % )) [ 1d 33 1 2 22 3 ]) Diff Index: grunt.js =================================================================== --- grunt.js (revision 31200) +++ grunt.js (working copy) @@ -12,6 +12,7 @@ module.exports = function (grunt) { + console.log( hello world ); // Project configuration. grunt.initConfig({ lint: { @@ -19,10 +20,6 @@ packages/services.web/{!(test)/**/,}*.js , packages/error/**/*.js ], - scripts: [ - grunt.js , - db/**/*.js - ], browser: [ packages/web/server.js , packages/web/server/**/*.js , Docker FROM ubuntu # Install vnc, xvfb in order to create a fake display and firefox RUN apt-get update apt-get install -y x11vnc xvfb firefox RUN mkdir ~/.vnc # Setup a password RUN x11vnc -storepasswd 1234 ~/.vnc/passwd # Autostart firefox (might not be the best way, but it does the trick) RUN bash -c echo firefox /.bashrc EXPOSE 5900 CMD [ x11vnc , -forever , -usepw , -create ] Elixir require Logger def accept ( port ) do { :ok , socket } = :gen_tcp . listen ( port , [ :binary , packet : :line , active : false , reuseaddr : true ]) Logger . info Accepting connections on port #{ port } loop_acceptor ( socket ) end defp loop_acceptor ( socket ) do { :ok , client } = :gen_tcp . accept ( socket ) serve ( client ) loop_acceptor ( socket ) end defp serve ( socket ) do socket | read_line () | write_line ( socket ) serve ( socket ) end defp read_line ( socket ) do { :ok , data } = :gen_tcp . recv ( socket , 0 ) data end defp write_line ( line , socket ) do :gen_tcp . send ( socket , line ) end Erlang circular ( Defs ) - [ { { Type , Base }, Fields } || { { Type , Base }, Fields } - Defs , Type == msg , circular ( Base , Defs ) ]. circular ( Base , Defs ) - Fields = proplists : get_value ({ msg , Base }, Defs ), circular ( Defs , Fields , [ Base ]). circular (_ Defs , [], _ Path ) - false ; circular ( Defs , [ Field | Fields ], Path ) - case Field #field.type of { msg , Type } - case lists : member ( Type , Path ) of false - Children = proplists : get_value ({ msg , Type }, Defs ), case circular ( Defs , Children , [ Type | Path ]) of false - circular ( Defs , Fields , Path ); true - true end ; true - Type == lists : last ( Path ) andalso ( length ( Path ) == 1 orelse not is_tree ( Path )) end ; _ - circular ( Defs , Fields , Path ) end . F #35 ; /// Asynchronously download retangles from the server /// and decode the JSON format to F# Rectangle record let [ Js ] getRectangles () : Async Rectangle [] = async { let req = XMLHttpRequest () req . Open ( POST , /get , true ) let! resp = req . AsyncSend () return JSON . parse ( resp ) } /// Repeatedly update rectangles after 0.5 sec let [ Js ] updateLoop () = async { while true do do ! Async . Sleep ( 500 ) let! rects = getRectangles () cleanRectangles () rects | Array . iter createRectangle } Go package main import fmt func counter ( id int , channel chan int , closer bool ) { for i := 0 ; i 10000000 ; i ++ { fmt . Println ( process , id , send , i ) channel - 1 } if closer { close ( channel ) } } func main () { channel := make ( chan int ) go counter ( 1 , channel , false ) go counter ( 2 , channel , true ) x := 0 // receiving data from channel for i := range channel { fmt . Println ( receiving ) x += i } fmt . Println ( x ) } HTML !doctype html html class = no-js lang = head meta charset = utf-8 meta http-equiv = x-ua-compatible content = ie=edge title HTML5 Boilerplate / title meta name = description content = meta name = viewport content = width=device-width, initial-scale=1 link rel = apple-touch-icon href = apple-touch-icon.png link rel = stylesheet href = css/normalize.css link rel = stylesheet href = css/main.css script src = js/vendor/modernizr-2.8.3.min.js / script / head body p Hello world! This is HTML5 Boilerplate. / p / body / html Java import java.util.LinkedList ; import java.lang.reflect.Array ; public class UnsortedHashSet E { private static final double LOAD_FACTOR_LIMIT = 0.7 ; private int size ; private LinkedList E [] con ; public UnsortedHashSet () { con = ( LinkedList E [])( new LinkedList [ 10 ]); } public boolean add ( E obj ) { int oldSize = size ; int index = Math . abs ( obj . hashCode ()) % con . length ; if ( con [ index ] == null ) con [ index ] = new LinkedList E (); if (! con [ index ]. contains ( obj )) { con [ index ]. add ( obj ); size ++; } if ( 1.0 * size / con . length LOAD_FACTOR_LIMIT ) resize (); return oldSize != size ; } private void resize () { UnsortedHashSet E temp = new UnsortedHashSet E (); temp . con = ( LinkedList E [])( new LinkedList [ con . length * 2 + 1 ]); for ( int i = 0 ; i con . length ; i ++) { if ( con [ i ] != null ) for ( E e : con [ i ]) temp . add ( e ); } con = temp . con ; } public int size () { return size ; } } JavaScript var Math = require ( lib/math ); var _extends = function ( target ) { for ( var i = 1 ; i arguments . length ; i ++ ) { var source = arguments [ i ]; for ( var key in source ) { target [ key ] = source [ key ]; } } return target ; }; var e = exports . e = 2.71828182846 ; exports [ default ] = function ( x ) { return Math . exp ( x ); }; module . exports = _extends ( exports [ default ], exports ); JSON { name : mkdocs-material , version : 0.2.4 , description : A Material Design theme for MkDocs , homepage : http://squidfunk.github.io/mkdocs-material/ , authors : [ squidfunk martin.donath@squidfunk.com ], license : MIT , main : Gulpfile.js , scripts : { start : ./node_modules/.bin/gulp watch --mkdocs , build : ./node_modules/.bin/gulp build --production } ... } Julia using MXNet mlp = @mx . chain mx . Variable ( : data ) = mx . FullyConnected ( name =: fc1 , num_hidden = 128 ) = mx . Activation ( name =: relu1 , act_type =: relu ) = mx . FullyConnected ( name =: fc2 , num_hidden = 64 ) = mx . Activation ( name =: relu2 , act_type =: relu ) = mx . FullyConnected ( name =: fc3 , num_hidden = 10 ) = mx . SoftmaxOutput ( name =: softmax ) # data provider batch_size = 100 include ( Pkg . dir ( MXNet , examples , mnist , mnist-data.jl )) train_provider , eval_provider = get_mnist_providers ( batch_size ) # setup model model = mx . FeedForward ( mlp , context = mx . cpu ()) # optimization algorithm optimizer = mx . SGD ( lr = 0.1 , momentum = 0.9 ) # fit parameters mx . fit ( model , optimizer , train_provider , n_epoch = 20 , eval_data = eval_provider ) Lua local ffi = require ( ffi ) ffi . cdef [[ void Sleep(int ms); int poll(struct pollfd *fds, unsigned long nfds, int timeout); ]] local sleep if ffi . os == Windows then function sleep ( s ) ffi . C . Sleep ( s * 1000 ) end else function sleep ( s ) ffi . C . poll ( nil , 0 , s * 1000 ) end end for i = 1 , 160 do io.write ( . ); io.flush () sleep ( 0.01 ) end io.write ( \\n ) MySQL SELECT Employees . EmployeeID , Employees . Name , Employees . Salary , Manager . Name AS Manager FROM Employees LEFT JOIN Employees AS Manager ON Employees . ManagerID = Manager . EmployeeID WHERE Employees . EmployeeID = 087652 ; PHP ?php // src/AppBundle/Controller/LuckyController.php namespace AppBundle\\Controller ; use Sensio\\Bundle\\FrameworkExtraBundle\\Configuration\\Route ; use Symfony\\Component\\HttpFoundation\\Response ; class LuckyController { /** * @Route( /lucky/number ) */ public function numberAction () { $number = mt_rand ( 0 , 100 ); return new Response ( html body Lucky number: . $number . /body /html ); } } Protocol Buffers syntax = proto2 ; package caffe ; // Specifies the shape (dimensions) of a Blob. message BlobShape { repeated int64 dim = 1 [ packed = true ]; } message BlobProto { optional BlobShape shape = 7 ; repeated float data = 5 [ packed = true ]; repeated float diff = 6 [ packed = true ]; // 4D dimensions -- deprecated. Use shape instead. optional int32 num = 1 [ default = 0 ]; optional int32 channels = 2 [ default = 0 ]; optional int32 height = 3 [ default = 0 ]; optional int32 width = 4 [ default = 0 ]; } Python A very simple MNIST classifier. See extensive documentation at http://tensorflow.org/tutorials/mnist/beginners/index.md from __future__ import absolute_import from __future__ import division from __future__ import print_function # Import data from tensorflow.examples.tutorials.mnist import input_data import tensorflow as tf flags = tf . app . flags FLAGS = flags . FLAGS flags . DEFINE_string ( data_dir , /tmp/data/ , Directory for storing data ) mnist = input_data . read_data_sets ( FLAGS . data_dir , one_hot = True ) sess = tf . InteractiveSession () # Create the model x = tf . placeholder ( tf . float32 , [ None , 784 ]) W = tf . Variable ( tf . zeros ([ 784 , 10 ])) b = tf . Variable ( tf . zeros ([ 10 ])) y = tf . nn . softmax ( tf . matmul ( x , W ) + b ) Ruby require finity/event require finity/machine require finity/state require finity/transition require finity/version module Finity class InvalidCallback StandardError ; end class MissingCallback StandardError ; end class InvalidState StandardError ; end # Class methods to be injected into the including class upon inclusion. module ClassMethods # Instantiate a new state machine for the including class by accepting a # block with state and event (and subsequent transition) definitions. def finity options = {}, block @finity ||= Machine . new self , options , block end # Return the names of all registered states. def states @finity . states . map { | name , _ | name } end # Return the names of all registered events. def events @finity . events . map { | name , _ | name } end end # Inject methods into the including class upon inclusion. def self . included base base . extend ClassMethods end end XML ?xml version= 1.0 encoding= UTF-8 ? !DOCTYPE mainTag SYSTEM some.dtd [ENTITY % entity] ?oxygen RNGSchema= some.rng type= xml ? xs:main-Tag xmlns:xs= http://www.w3.org/2001/XMLSchema !-- This is a sample comment -- childTag attribute= Quoted Value another-attribute= Single quoted value a-third-attribute= 123 withTextContent Some text content /withTextContent withEntityContent Some text content with lt; entities gt; and mentioning uint8_t and int32_t /withEntityContent otherTag attribute= Single quoted Value / /childTag ![CDATA[ some CData ]] /main-Tag","title":"CodeHilite"},{"location":"extensions/codehilite/#codehilite","text":"CodeHilite is an extension that adds syntax highlighting to code blocks and is included in the standard Markdown library. The highlighting process is executed during compilation of the Markdown file. Syntax highlighting not working? Please ensure that Pygments is installed. See the next section for further directions on how to set up Pygments or use the official Docker image with all dependencies pre-installed.","title":"CodeHilite"},{"location":"extensions/codehilite/#installation","text":"CodeHilite parses code blocks and wraps them in pre tags. If Pygments is installed, which is a generic syntax highlighter with support for over 300 languages , CodeHilite will also highlight the code block. Pygments can be installed with the following command: pip install pygments To enable CodeHilite, add the following lines to your mkdocs.yml : markdown_extensions : - codehilite","title":"Installation"},{"location":"extensions/codehilite/#usage","text":"","title":"Usage"},{"location":"extensions/codehilite/#specifying-the-language","text":"The CodeHilite extension uses the same syntax as regular Markdown code blocks, but needs to know the language of the code block. This can be done in three different ways.","title":"Specifying the language"},{"location":"extensions/codehilite/#via-markdown-syntax-recommended","text":"In Markdown, code blocks can be opened and closed by writing three backticks on separate lines. To add code highlighting to those blocks, the easiest way is to specify the language directly after the opening block. Example: ``` python import tensorflow as tf ``` Result: import tensorflow as tf","title":"via Markdown syntax recommended"},{"location":"extensions/codehilite/#via-shebang","text":"Alternatively, if the first line of a code block contains a shebang, the language is derived from the path referenced in the shebang. This will only work for code blocks that are indented using four spaces, not for those encapsulated in three backticks. Example: #!/usr/bin/python import tensorflow as tf Result: #!/usr/bin/python import tensorflow as tf","title":"via Shebang"},{"location":"extensions/codehilite/#via-three-colons","text":"If the first line starts with three colons followed by a language identifier, the first line is stripped. This will only work for code blocks that are indented using four spaces, not for those encapsulated in three backticks. Example: :::python import tensorflow as tf Result: import tensorflow as tf","title":"via three colons"},{"location":"extensions/codehilite/#adding-line-numbers","text":"Line numbers can be added by enabling the linenums flag in your mkdocs.yml : markdown_extensions : - codehilite : linenums : true Example: ``` python Bubble sort def bubble_sort(items): for i in range(len(items)): for j in range(len(items) - 1 - i): if items[j] items[j + 1]: items[j], items[j + 1] = items[j + 1], items[j] ``` Result: 1 2 3 4 5 6 Bubble sort def bubble_sort ( items ): for i in range ( len ( items )): for j in range ( len ( items ) - 1 - i ): if items [ j ] items [ j + 1 ]: items [ j ], items [ j + 1 ] = items [ j + 1 ], items [ j ]","title":"Adding line numbers"},{"location":"extensions/codehilite/#grouping-code-blocks","text":"The SuperFences extension which is part of the PyMdown Extensions package adds support for grouping code blocks with tabs. This is especially useful for documenting projects with multiple language bindings. Example: ``` bash tab= Bash #!/bin/bash echo Hello world! ``` ``` c tab= C #include stdio.h int main(void) { printf( Hello world!\\n ); } ``` ``` c++ tab= C++ #include iostream int main() { std::cout Hello world! std::endl; return 0; } ``` ``` c# tab= C# using System; class Program { static void Main(string[] args) { Console.WriteLine( Hello world! ); } } ``` Result: Bash #!/bin/bash echo Hello world! C #include stdio.h int main ( void ) { printf ( Hello world! \\n ); } C++ #include iostream int main () { std :: cout Hello world! std :: endl ; return 0 ; } C# using System ; class Program { static void Main ( string [] args ) { Console . WriteLine ( Hello world! ); } }","title":"Grouping code blocks"},{"location":"extensions/codehilite/#highlighting-specific-lines","text":"Specific lines can be highlighted by passing the line numbers to the hl_lines argument placed right after the language identifier. Line counts start at 1. Example: ``` python hl_lines= 3 4 Bubble sort def bubble_sort(items): for i in range(len(items)): for j in range(len(items) - 1 - i): if items[j] items[j + 1]: items[j], items[j + 1] = items[j + 1], items[j] ``` Result: 1 2 3 4 5 6 Bubble sort def bubble_sort ( items ): for i in range ( len ( items )): for j in range ( len ( items ) - 1 - i ): if items [ j ] items [ j + 1 ]: items [ j ], items [ j + 1 ] = items [ j + 1 ], items [ j ]","title":"Highlighting specific lines"},{"location":"extensions/codehilite/#supported-languages-excerpt","text":"CodeHilite uses Pygments , a generic syntax highlighter with support for over 300 languages , so the following list of examples is just an excerpt.","title":"Supported languages excerpt"},{"location":"extensions/codehilite/#bash","text":"#!/bin/bash for OPT in $@ do case $OPT in -f ) canonicalize = 1 ;; -n ) switchlf = -n ;; esac done # readlink -f function __readlink_f { target = $1 while test -n $target ; do filepath = $target cd ` dirname $filepath ` target = ` readlink $filepath ` done /bin/echo $switchlf ` pwd -P ` / ` basename $filepath ` } if [ ! $canonicalize ] ; then readlink $switchlf $@ else for file in $@ do case $file in -* ) ;; * ) __readlink_f $file ;; esac done fi exit $?","title":"Bash"},{"location":"extensions/codehilite/#c","text":"extern size_t pb_varint_scan ( const uint8_t data [], size_t left ) { assert ( data left ); left = left 10 ? 10 : left ; #ifdef __SSE2__ /* Mapping: remaining bytes == bitmask */ static const int mask_map [] = { 0x0000 , 0x0001 , 0x0003 , 0x0007 , 0x000F , 0x001F , 0x003F , 0x007F , 0x00FF , 0x01FF , 0x03FF }; /* Load buffer into 128-bit integer and create high-bit mask */ __m128i temp = _mm_loadu_si128 (( const __m128i * ) data ); __m128i high = _mm_set1_epi8 ( 0x80 ); /* Intersect and extract mask with high-bits set */ int mask = _mm_movemask_epi8 ( _mm_and_si128 ( temp , high )); mask = ( mask mask_map [ left ]) ^ mask_map [ left ]; /* Count trailing zeroes */ return mask ? __builtin_ctz ( mask ) + 1 : 0 ; #else /* Linear scan */ size_t size = 0 ; while ( data [ size ++ ] 0x80 ) if ( !-- left ) return 0 ; return size ; #endif /* __SSE2__ */ }","title":"C"},{"location":"extensions/codehilite/#c_1","text":"Extension :: Extension ( const Descriptor * descriptor , const Descriptor * scope ) : descriptor_ ( descriptor ), scope_ ( scope ) { /* Extract full name for signature */ variables_ [ signature ] = descriptor_ - full_name (); /* Prepare message symbol */ variables_ [ message ] = StringReplace ( variables_ [ signature ], . , _ , true ); LowerString ( ( variables_ [ message ])); /* Suffix scope to identifiers, if given */ string suffix ( ); if ( scope_ ) { suffix = scope_ - full_name (); /* Check if the base and extension types are in the same package */ if ( ! scope_ - file () - package (). compare ( descriptor_ - file () - package ())) suffix = StripPrefixString ( suffix , scope_ - file () - package () + . ); /* Append to signature */ variables_ [ signature ] += .[ + suffix + ] ; suffix = _ + suffix ; } /* Prepare extension symbol */ variables_ [ extension ] = StringReplace ( suffix , . , _ , true ); LowerString ( ( variables_ [ extension ])); }","title":"C++"},{"location":"extensions/codehilite/#c35","text":"public static void Send ( Socket socket , byte [] buffer , int offset , int size , int timeout ) { int startTickCount = Environment . TickCount ; int sent = 0 ; do { if ( Environment . TickCount startTickCount + timeout ) throw new Exception ( Timeout. ); try { sent += socket . Send ( buffer , offset + sent , size - sent , SocketFlags . None ); } catch ( SocketException ex ) { if ( ex . SocketErrorCode == SocketError . WouldBlock || ex . SocketErrorCode == SocketError . IOPending || ex . SocketErrorCode == SocketError . NoBufferSpaceAvailable ) { /* Socket buffer is probably full, wait and try again */ Thread . Sleep ( 30 ); } else { throw ex ; } } } while ( sent size ); }","title":"C&#35;"},{"location":"extensions/codehilite/#clojure","text":"( clojure-version ) ( defn partition-when [ f ] ( fn [ rf ] ( let [ a ( java.util.ArrayList. ) fval ( volatile! false )] ( fn ([] ( rf )) ([ result ] ( let [ result ( if ( .isEmpty a ) result ( let [ v ( vec ( .toArray a ))] ;; Clear first ( .clear a ) ( unreduced ( rf result v ))))] ( rf result ))) ([ result input ] ( if-not ( and ( f input ) @ fval ) ( do ( vreset! fval true ) ( .add a input ) result ) ( let [ v ( vec ( .toArray a ))] ( .clear a ) ( let [ ret ( rf result v )] ( when-not ( reduced? ret ) ( .add a input )) ret )))))))) ( into [] ( partition-when # ( .startsWith % )) [ 1d 33 1 2 22 3 ])","title":"Clojure"},{"location":"extensions/codehilite/#diff","text":"Index: grunt.js =================================================================== --- grunt.js (revision 31200) +++ grunt.js (working copy) @@ -12,6 +12,7 @@ module.exports = function (grunt) { + console.log( hello world ); // Project configuration. grunt.initConfig({ lint: { @@ -19,10 +20,6 @@ packages/services.web/{!(test)/**/,}*.js , packages/error/**/*.js ], - scripts: [ - grunt.js , - db/**/*.js - ], browser: [ packages/web/server.js , packages/web/server/**/*.js ,","title":"Diff"},{"location":"extensions/codehilite/#docker","text":"FROM ubuntu # Install vnc, xvfb in order to create a fake display and firefox RUN apt-get update apt-get install -y x11vnc xvfb firefox RUN mkdir ~/.vnc # Setup a password RUN x11vnc -storepasswd 1234 ~/.vnc/passwd # Autostart firefox (might not be the best way, but it does the trick) RUN bash -c echo firefox /.bashrc EXPOSE 5900 CMD [ x11vnc , -forever , -usepw , -create ]","title":"Docker"},{"location":"extensions/codehilite/#elixir","text":"require Logger def accept ( port ) do { :ok , socket } = :gen_tcp . listen ( port , [ :binary , packet : :line , active : false , reuseaddr : true ]) Logger . info Accepting connections on port #{ port } loop_acceptor ( socket ) end defp loop_acceptor ( socket ) do { :ok , client } = :gen_tcp . accept ( socket ) serve ( client ) loop_acceptor ( socket ) end defp serve ( socket ) do socket | read_line () | write_line ( socket ) serve ( socket ) end defp read_line ( socket ) do { :ok , data } = :gen_tcp . recv ( socket , 0 ) data end defp write_line ( line , socket ) do :gen_tcp . send ( socket , line ) end","title":"Elixir"},{"location":"extensions/codehilite/#erlang","text":"circular ( Defs ) - [ { { Type , Base }, Fields } || { { Type , Base }, Fields } - Defs , Type == msg , circular ( Base , Defs ) ]. circular ( Base , Defs ) - Fields = proplists : get_value ({ msg , Base }, Defs ), circular ( Defs , Fields , [ Base ]). circular (_ Defs , [], _ Path ) - false ; circular ( Defs , [ Field | Fields ], Path ) - case Field #field.type of { msg , Type } - case lists : member ( Type , Path ) of false - Children = proplists : get_value ({ msg , Type }, Defs ), case circular ( Defs , Children , [ Type | Path ]) of false - circular ( Defs , Fields , Path ); true - true end ; true - Type == lists : last ( Path ) andalso ( length ( Path ) == 1 orelse not is_tree ( Path )) end ; _ - circular ( Defs , Fields , Path ) end .","title":"Erlang"},{"location":"extensions/codehilite/#f35","text":"/// Asynchronously download retangles from the server /// and decode the JSON format to F# Rectangle record let [ Js ] getRectangles () : Async Rectangle [] = async { let req = XMLHttpRequest () req . Open ( POST , /get , true ) let! resp = req . AsyncSend () return JSON . parse ( resp ) } /// Repeatedly update rectangles after 0.5 sec let [ Js ] updateLoop () = async { while true do do ! Async . Sleep ( 500 ) let! rects = getRectangles () cleanRectangles () rects | Array . iter createRectangle }","title":"F&#35;"},{"location":"extensions/codehilite/#go","text":"package main import fmt func counter ( id int , channel chan int , closer bool ) { for i := 0 ; i 10000000 ; i ++ { fmt . Println ( process , id , send , i ) channel - 1 } if closer { close ( channel ) } } func main () { channel := make ( chan int ) go counter ( 1 , channel , false ) go counter ( 2 , channel , true ) x := 0 // receiving data from channel for i := range channel { fmt . Println ( receiving ) x += i } fmt . Println ( x ) }","title":"Go"},{"location":"extensions/codehilite/#html","text":"!doctype html html class = no-js lang = head meta charset = utf-8 meta http-equiv = x-ua-compatible content = ie=edge title HTML5 Boilerplate / title meta name = description content = meta name = viewport content = width=device-width, initial-scale=1 link rel = apple-touch-icon href = apple-touch-icon.png link rel = stylesheet href = css/normalize.css link rel = stylesheet href = css/main.css script src = js/vendor/modernizr-2.8.3.min.js / script / head body p Hello world! This is HTML5 Boilerplate. / p / body / html","title":"HTML"},{"location":"extensions/codehilite/#java","text":"import java.util.LinkedList ; import java.lang.reflect.Array ; public class UnsortedHashSet E { private static final double LOAD_FACTOR_LIMIT = 0.7 ; private int size ; private LinkedList E [] con ; public UnsortedHashSet () { con = ( LinkedList E [])( new LinkedList [ 10 ]); } public boolean add ( E obj ) { int oldSize = size ; int index = Math . abs ( obj . hashCode ()) % con . length ; if ( con [ index ] == null ) con [ index ] = new LinkedList E (); if (! con [ index ]. contains ( obj )) { con [ index ]. add ( obj ); size ++; } if ( 1.0 * size / con . length LOAD_FACTOR_LIMIT ) resize (); return oldSize != size ; } private void resize () { UnsortedHashSet E temp = new UnsortedHashSet E (); temp . con = ( LinkedList E [])( new LinkedList [ con . length * 2 + 1 ]); for ( int i = 0 ; i con . length ; i ++) { if ( con [ i ] != null ) for ( E e : con [ i ]) temp . add ( e ); } con = temp . con ; } public int size () { return size ; } }","title":"Java"},{"location":"extensions/codehilite/#javascript","text":"var Math = require ( lib/math ); var _extends = function ( target ) { for ( var i = 1 ; i arguments . length ; i ++ ) { var source = arguments [ i ]; for ( var key in source ) { target [ key ] = source [ key ]; } } return target ; }; var e = exports . e = 2.71828182846 ; exports [ default ] = function ( x ) { return Math . exp ( x ); }; module . exports = _extends ( exports [ default ], exports );","title":"JavaScript"},{"location":"extensions/codehilite/#json","text":"{ name : mkdocs-material , version : 0.2.4 , description : A Material Design theme for MkDocs , homepage : http://squidfunk.github.io/mkdocs-material/ , authors : [ squidfunk martin.donath@squidfunk.com ], license : MIT , main : Gulpfile.js , scripts : { start : ./node_modules/.bin/gulp watch --mkdocs , build : ./node_modules/.bin/gulp build --production } ... }","title":"JSON"},{"location":"extensions/codehilite/#julia","text":"using MXNet mlp = @mx . chain mx . Variable ( : data ) = mx . FullyConnected ( name =: fc1 , num_hidden = 128 ) = mx . Activation ( name =: relu1 , act_type =: relu ) = mx . FullyConnected ( name =: fc2 , num_hidden = 64 ) = mx . Activation ( name =: relu2 , act_type =: relu ) = mx . FullyConnected ( name =: fc3 , num_hidden = 10 ) = mx . SoftmaxOutput ( name =: softmax ) # data provider batch_size = 100 include ( Pkg . dir ( MXNet , examples , mnist , mnist-data.jl )) train_provider , eval_provider = get_mnist_providers ( batch_size ) # setup model model = mx . FeedForward ( mlp , context = mx . cpu ()) # optimization algorithm optimizer = mx . SGD ( lr = 0.1 , momentum = 0.9 ) # fit parameters mx . fit ( model , optimizer , train_provider , n_epoch = 20 , eval_data = eval_provider )","title":"Julia"},{"location":"extensions/codehilite/#lua","text":"local ffi = require ( ffi ) ffi . cdef [[ void Sleep(int ms); int poll(struct pollfd *fds, unsigned long nfds, int timeout); ]] local sleep if ffi . os == Windows then function sleep ( s ) ffi . C . Sleep ( s * 1000 ) end else function sleep ( s ) ffi . C . poll ( nil , 0 , s * 1000 ) end end for i = 1 , 160 do io.write ( . ); io.flush () sleep ( 0.01 ) end io.write ( \\n )","title":"Lua"},{"location":"extensions/codehilite/#mysql","text":"SELECT Employees . EmployeeID , Employees . Name , Employees . Salary , Manager . Name AS Manager FROM Employees LEFT JOIN Employees AS Manager ON Employees . ManagerID = Manager . EmployeeID WHERE Employees . EmployeeID = 087652 ;","title":"MySQL"},{"location":"extensions/codehilite/#php","text":"?php // src/AppBundle/Controller/LuckyController.php namespace AppBundle\\Controller ; use Sensio\\Bundle\\FrameworkExtraBundle\\Configuration\\Route ; use Symfony\\Component\\HttpFoundation\\Response ; class LuckyController { /** * @Route( /lucky/number ) */ public function numberAction () { $number = mt_rand ( 0 , 100 ); return new Response ( html body Lucky number: . $number . /body /html ); } }","title":"PHP"},{"location":"extensions/codehilite/#protocol-buffers","text":"syntax = proto2 ; package caffe ; // Specifies the shape (dimensions) of a Blob. message BlobShape { repeated int64 dim = 1 [ packed = true ]; } message BlobProto { optional BlobShape shape = 7 ; repeated float data = 5 [ packed = true ]; repeated float diff = 6 [ packed = true ]; // 4D dimensions -- deprecated. Use shape instead. optional int32 num = 1 [ default = 0 ]; optional int32 channels = 2 [ default = 0 ]; optional int32 height = 3 [ default = 0 ]; optional int32 width = 4 [ default = 0 ]; }","title":"Protocol Buffers"},{"location":"extensions/codehilite/#python","text":"A very simple MNIST classifier. See extensive documentation at http://tensorflow.org/tutorials/mnist/beginners/index.md from __future__ import absolute_import from __future__ import division from __future__ import print_function # Import data from tensorflow.examples.tutorials.mnist import input_data import tensorflow as tf flags = tf . app . flags FLAGS = flags . FLAGS flags . DEFINE_string ( data_dir , /tmp/data/ , Directory for storing data ) mnist = input_data . read_data_sets ( FLAGS . data_dir , one_hot = True ) sess = tf . InteractiveSession () # Create the model x = tf . placeholder ( tf . float32 , [ None , 784 ]) W = tf . Variable ( tf . zeros ([ 784 , 10 ])) b = tf . Variable ( tf . zeros ([ 10 ])) y = tf . nn . softmax ( tf . matmul ( x , W ) + b )","title":"Python"},{"location":"extensions/codehilite/#ruby","text":"require finity/event require finity/machine require finity/state require finity/transition require finity/version module Finity class InvalidCallback StandardError ; end class MissingCallback StandardError ; end class InvalidState StandardError ; end # Class methods to be injected into the including class upon inclusion. module ClassMethods # Instantiate a new state machine for the including class by accepting a # block with state and event (and subsequent transition) definitions. def finity options = {}, block @finity ||= Machine . new self , options , block end # Return the names of all registered states. def states @finity . states . map { | name , _ | name } end # Return the names of all registered events. def events @finity . events . map { | name , _ | name } end end # Inject methods into the including class upon inclusion. def self . included base base . extend ClassMethods end end","title":"Ruby"},{"location":"extensions/codehilite/#xml","text":"?xml version= 1.0 encoding= UTF-8 ? !DOCTYPE mainTag SYSTEM some.dtd [ENTITY % entity] ?oxygen RNGSchema= some.rng type= xml ? xs:main-Tag xmlns:xs= http://www.w3.org/2001/XMLSchema !-- This is a sample comment -- childTag attribute= Quoted Value another-attribute= Single quoted value a-third-attribute= 123 withTextContent Some text content /withTextContent withEntityContent Some text content with lt; entities gt; and mentioning uint8_t and int32_t /withEntityContent otherTag attribute= Single quoted Value / /childTag ![CDATA[ some CData ]] /main-Tag","title":"XML"},{"location":"extensions/footnotes/","text":"Footnotes Footnotes is another extension included in the standard Markdown library. As the name says, it adds the ability to add footnotes to your documentation. Installation Add the following lines to your mkdocs.yml : markdown_extensions : - footnotes Usage The markup for footnotes is similar to the standard Markdown markup for links. A reference is inserted in the text, which can then be defined at any point in the document. Inserting the reference The footnote reference is enclosed in square brackets and starts with a caret, followed by an arbitrary label which may contain numeric identifiers [1, 2, 3, ...] or names [Granovetter et al. 1998]. The rendered references are always consecutive superscripted numbers. Example: Lorem ipsum[^1] dolor sit amet, consectetur adipiscing elit.[^2] Result: Lorem ipsum 1 dolor sit amet, consectetur adipiscing elit. 2 Inserting the content The footnote content is also declared with a label, which must match the label used for the footnote reference. It can be inserted at an arbitrary position in the document and is always rendered at the bottom of the page. Furthermore, a backlink is automatically added to the footnote reference. on a single line Short statements can be written on the same line. Example: [^1]: Lorem ipsum dolor sit amet, consectetur adipiscing elit. Result: Jump to footnote at the bottom of the page on multiple lines Paragraphs should be written on the next line. As with all Markdown blocks, the content must be indented by four spaces. Example: [^2]: Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Result: Jump to footnote at the bottom of the page Lorem ipsum dolor sit amet, consectetur adipiscing elit. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.","title":"Footnotes"},{"location":"extensions/footnotes/#footnotes","text":"Footnotes is another extension included in the standard Markdown library. As the name says, it adds the ability to add footnotes to your documentation.","title":"Footnotes"},{"location":"extensions/footnotes/#installation","text":"Add the following lines to your mkdocs.yml : markdown_extensions : - footnotes","title":"Installation"},{"location":"extensions/footnotes/#usage","text":"The markup for footnotes is similar to the standard Markdown markup for links. A reference is inserted in the text, which can then be defined at any point in the document.","title":"Usage"},{"location":"extensions/footnotes/#inserting-the-reference","text":"The footnote reference is enclosed in square brackets and starts with a caret, followed by an arbitrary label which may contain numeric identifiers [1, 2, 3, ...] or names [Granovetter et al. 1998]. The rendered references are always consecutive superscripted numbers. Example: Lorem ipsum[^1] dolor sit amet, consectetur adipiscing elit.[^2] Result: Lorem ipsum 1 dolor sit amet, consectetur adipiscing elit. 2","title":"Inserting the reference"},{"location":"extensions/footnotes/#inserting-the-content","text":"The footnote content is also declared with a label, which must match the label used for the footnote reference. It can be inserted at an arbitrary position in the document and is always rendered at the bottom of the page. Furthermore, a backlink is automatically added to the footnote reference.","title":"Inserting the content"},{"location":"extensions/footnotes/#on-a-single-line","text":"Short statements can be written on the same line. Example: [^1]: Lorem ipsum dolor sit amet, consectetur adipiscing elit. Result: Jump to footnote at the bottom of the page","title":"on a single line"},{"location":"extensions/footnotes/#on-multiple-lines","text":"Paragraphs should be written on the next line. As with all Markdown blocks, the content must be indented by four spaces. Example: [^2]: Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Result: Jump to footnote at the bottom of the page Lorem ipsum dolor sit amet, consectetur adipiscing elit. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.","title":"on multiple lines"},{"location":"extensions/metadata/","text":"Metadata The Metadata extension makes it possible to add metadata to a document which gives more control over the theme in a page-specific context. Installation Add the following lines to your mkdocs.yml : markdown_extensions : - meta Usage Metadata is written as a series of key-value pairs at the beginning of the Markdown document, delimited by a blank line which ends the metadata context. Naturally, the metadata is stripped from the document before rendering the actual page content and made available to the theme. Example: title: Lorem ipsum dolor sit amet description: Nullam urna elit, malesuada eget finibus ut, ac tortor. path: path/to/file source: file.js # Headline ... See the next section which covers the metadata that is supported by Material. Setting a hero text Material exposes a simple text-only page-local hero via Metadata, as you can see on the current page when you scroll to the top. It's as simple as: hero: Metadata enables hero teaser texts Linking sources When a document is related to a specific set of source files and the repo_url is defined inside the project's mkdocs.yml , the files can be linked using the source key: source: file.js The filename is appended to the repo_url set in your mkdocs.yml , but can be prefixed with a path to ensure correct path resolving: Example: path: tree/master/docs/extensions source: metadata.md Result: See the source section for the resulting output. Redirecting to another page It's sometimes necessary to move documents around in the navigation tree and redirect user from the old URL to the new one. The redirect meta-tag allows to create a redirection from the current document to the address specified in the tag. For instance, if your document contains: redirect: /new/url accessing that document's URL will automatically redirect to /new/url . Overrides Page title The page title can be overridden on a per-document level: title: Lorem ipsum dolor sit amet This will set the title tag inside the document head for the current page to the provided value. It will also override the default behavior of Material for MkDocs which appends the site title using a dash as a separator to the page title. Page description The page description can also be overridden on a per-document level: description : Nullam urna elit, malesuada eget finibus ut, ac tortor. This will set the meta tag containing the site description inside the document head for the current page to the provided value. Disqus As describe in the getting started guide , the Disqus comments section can be enabled on a per-document level: disqus: your-shortname Disqus can be disabled for a specific page by setting it to an empty value: disqus:","title":"Metadata"},{"location":"extensions/metadata/#metadata","text":"The Metadata extension makes it possible to add metadata to a document which gives more control over the theme in a page-specific context.","title":"Metadata"},{"location":"extensions/metadata/#installation","text":"Add the following lines to your mkdocs.yml : markdown_extensions : - meta","title":"Installation"},{"location":"extensions/metadata/#usage","text":"Metadata is written as a series of key-value pairs at the beginning of the Markdown document, delimited by a blank line which ends the metadata context. Naturally, the metadata is stripped from the document before rendering the actual page content and made available to the theme. Example: title: Lorem ipsum dolor sit amet description: Nullam urna elit, malesuada eget finibus ut, ac tortor. path: path/to/file source: file.js # Headline ... See the next section which covers the metadata that is supported by Material.","title":"Usage"},{"location":"extensions/metadata/#setting-a-hero-text","text":"Material exposes a simple text-only page-local hero via Metadata, as you can see on the current page when you scroll to the top. It's as simple as: hero: Metadata enables hero teaser texts","title":"Setting a hero text"},{"location":"extensions/metadata/#linking-sources","text":"When a document is related to a specific set of source files and the repo_url is defined inside the project's mkdocs.yml , the files can be linked using the source key: source: file.js The filename is appended to the repo_url set in your mkdocs.yml , but can be prefixed with a path to ensure correct path resolving: Example: path: tree/master/docs/extensions source: metadata.md Result: See the source section for the resulting output.","title":"Linking sources"},{"location":"extensions/metadata/#redirecting-to-another-page","text":"It's sometimes necessary to move documents around in the navigation tree and redirect user from the old URL to the new one. The redirect meta-tag allows to create a redirection from the current document to the address specified in the tag. For instance, if your document contains: redirect: /new/url accessing that document's URL will automatically redirect to /new/url .","title":"Redirecting to another page"},{"location":"extensions/metadata/#overrides","text":"","title":"Overrides"},{"location":"extensions/metadata/#page-title","text":"The page title can be overridden on a per-document level: title: Lorem ipsum dolor sit amet This will set the title tag inside the document head for the current page to the provided value. It will also override the default behavior of Material for MkDocs which appends the site title using a dash as a separator to the page title.","title":"Page title"},{"location":"extensions/metadata/#page-description","text":"The page description can also be overridden on a per-document level: description : Nullam urna elit, malesuada eget finibus ut, ac tortor. This will set the meta tag containing the site description inside the document head for the current page to the provided value.","title":"Page description"},{"location":"extensions/metadata/#disqus","text":"As describe in the getting started guide , the Disqus comments section can be enabled on a per-document level: disqus: your-shortname Disqus can be disabled for a specific page by setting it to an empty value: disqus:","title":"Disqus"},{"location":"extensions/permalinks/","text":"Permalinks Permalinks are a feature of the Table of Contents extension, which is part of the standard Markdown library. The extension inserts an anchor at the end of each headline, which makes it possible to directly link to a subpart of the document. Installation To enable permalinks, add the following to your mkdocs.yml : markdown_extensions : - toc : permalink : true This will add a link containing the paragraph symbol \u00b6 at the end of each headline (exactly like on the page you're currently viewing), which the Material theme will make appear on hover. In order to change the text of the permalink, a string can be passed, e.g.: markdown_extensions: - toc: permalink: Link Usage When enabled, permalinks are inserted automatically.","title":"Permalinks"},{"location":"extensions/permalinks/#permalinks","text":"Permalinks are a feature of the Table of Contents extension, which is part of the standard Markdown library. The extension inserts an anchor at the end of each headline, which makes it possible to directly link to a subpart of the document.","title":"Permalinks"},{"location":"extensions/permalinks/#installation","text":"To enable permalinks, add the following to your mkdocs.yml : markdown_extensions : - toc : permalink : true This will add a link containing the paragraph symbol \u00b6 at the end of each headline (exactly like on the page you're currently viewing), which the Material theme will make appear on hover. In order to change the text of the permalink, a string can be passed, e.g.: markdown_extensions: - toc: permalink: Link","title":"Installation"},{"location":"extensions/permalinks/#usage","text":"When enabled, permalinks are inserted automatically.","title":"Usage"},{"location":"extensions/pymdown/","text":"PyMdown Extensions PyMdown Extensions is a collection of Markdown extensions that add some great features to the standard Markdown library. For this reason, the installation of this package is highly recommended as it's well-integrated with the Material theme. Installation The PyMdown Extensions package can be installed with the following command: pip install pymdown-extensions The following list of extensions that are part of the PyMdown Extensions package are recommended to be used together with the Material theme: markdown_extensions : - pymdownx.arithmatex - pymdownx.betterem : smart_enable : all - pymdownx.caret - pymdownx.critic - pymdownx.details - pymdownx.emoji : emoji_generator : !!python/name:pymdownx.emoji.to_svg - pymdownx.inlinehilite - pymdownx.magiclink - pymdownx.mark - pymdownx.smartsymbols - pymdownx.superfences - pymdownx.tasklist : custom_checkbox : true - pymdownx.tilde Usage Arithmatex MathJax Arithmatex integrates Material with MathJax which parses block-style and inline equations written in TeX markup and outputs them in mathematical notation. See this thread for a short introduction and quick reference on how to write equations in TeX syntax. Besides activating the extension in the mkdocs.yml , the MathJax JavaScript runtime needs to be included. This must be done with additional JavaScript : extra_javascript : - https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML If you want to override the default MathJax configuration, you can do this by adding another JavaScript file before the MathJax runtime in extra_javascript which contains your MathJax configuration, e.g.: window . MathJax = { tex2jax : { inlineMath : [ [ \\\\( , \\\\) ] ], displayMath : [ [ \\\\[ , \\\\] ] ] }, TeX : { TagSide : right , TagIndent : .8em , MultLineWidth : 85% , equationNumbers : { autoNumber : AMS , }, unicode : { fonts : STIXGeneral, Arial Unicode MS } }, displayAlign : left , showProcessingMessages : false , messageStyle : none }; In your mkdocs.yml , include it with: extra_javascript : - javascripts/extra.js - https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML Blocks Blocks are enclosed in $$ ... $$ which are placed on separate lines. Example: $$ \\frac {n ! }{k !( n - k )! } = \\binom {n}{k} $$ Result: \\frac{n!}{k!(n-k)!} = \\binom{n}{k} \\frac{n!}{k!(n-k)!} = \\binom{n}{k} Inline Inline equations need to be enclosed in $ ... $ : Example: Lorem ipsum dolor sit amet: $ p ( x|y ) = \\frac {p ( y|x ) p ( x ) }{p ( y ) } $ Result: Lorem ipsum dolor sit amet: p(x|y) = \\frac{p(y|x)p(x)}{p(y)} p(x|y) = \\frac{p(y|x)p(x)}{p(y)} BetterEm BetterEm improves the handling of emphasis markup ( bold and italic ) within Markdown by providing a more sophisticated parser for better detecting start and end tokens. Read the documentation for usage notes . Caret Caret makes it possible to highlight inserted text . The portion of text that should be marked as added must be enclosed in two carets ^^...^^ . Critic Critic implements Critic Markup , a Markdown extension that enables the tracking of changes (additions, deletions and comments) on documents. During compilation of the Markdown document, changes can be rendered (default), accepted or rejected. Text can be deleted and replacement text added . This can also be combined into one a single operation. Highlighting is also possible and comments can be added inline . Formatting can also be applied to blocks, by putting the opening and closing tags on separate lines and adding new lines between the tags and the content. Details Details adds collapsible Admonition-style blocks which can contain arbitrary content using the HTML5 details and summary tags. Additionally, all Admonition qualifiers can be used, e.g. note , question , warning etc.: How many Prolog programmers does it take to change a lightbulb? Yes. Emoji Emoji adds the ability to insert a -load of emojis that we use in our daily lives. See the EmojiOne demo for a list of all available emojis. Happy scrolling Legal disclaimer Material has no affiliation with EmojiOne which is released under CC BY 4.0 . When including EmojiOne images or CSS, please read the EmojiOne license to ensure proper usage and attribution. InlineHilite InlineHilite adds support for inline code highlighting. It's useful for short snippets included within body copy, e.g. var test = 0 ; and can be achived by prefixing inline code with a shebang and language identifier, e.g. #!js . MagicLink MagicLink detects links in Markdown and auto-generates the necessary markup, so no special syntax is required. It auto-links http[s]:// and ftp:// links, as well as references to email addresses. Mark Mark adds the ability to highlight text like it was marked with a text marker . The portion of text that should be highlighted must be enclosed in two equal signs ==...== . SmartSymbols SmartSymbols converts markup for special characters into their corresponding symbols, e.g. arrows ( , , ), trademark and copyright symbols ( , , ) and fractions ( , , ...). SuperFences SuperFences provides the ability to nest code blocks under blockquotes, lists and other block elements, which the Fenced Code Blocks extension from the standard Markdown library doesn't parse correctly. SuperFences does also allow grouping code blocks with tabs . Tasklist Tasklist adds support for styled checkbox lists. This is useful for keeping track of tasks and showing what has been done and has yet to be done. Checkbox lists are like regular lists, but prefixed with [ ] for empty or [x] for filled checkboxes. Example: * [x] Lorem ipsum dolor sit amet, consectetur adipiscing elit * [x] Nulla lobortis egestas semper * [x] Curabitur elit nibh, euismod et ullamcorper at, iaculis feugiat est * [ ] Vestibulum convallis sit amet nisi a tincidunt * [x] In hac habitasse platea dictumst * [x] In scelerisque nibh non dolor mollis congue sed et metus * [x] Sed egestas felis quis elit dapibus, ac aliquet turpis mattis * [ ] Praesent sed risus massa * [ ] Aenean pretium efficitur erat, donec pharetra, ligula non scelerisque * [ ] Nulla vel eros venenatis, imperdiet enim id, faucibus nisi Result: Lorem ipsum dolor sit amet, consectetur adipiscing elit Nulla lobortis egestas semper Curabitur elit nibh, euismod et ullamcorper at, iaculis feugiat est Vestibulum convallis sit amet nisi a tincidunt In hac habitasse platea dictumst In scelerisque nibh non dolor mollis congue sed et metus Sed egestas felis quis elit dapibus, ac aliquet turpis mattis Praesent sed risus massa Aenean pretium efficitur erat, donec pharetra, ligula non scelerisque Nulla vel eros venenatis, imperdiet enim id, faucibus nisi Tilde Tilde provides an easy way to strike through cross out text. The portion of text that should be erased must be enclosed in two tildes ~~...~~ and the extension will take care of the rest.","title":"PyMdown"},{"location":"extensions/pymdown/#pymdown-extensions","text":"PyMdown Extensions is a collection of Markdown extensions that add some great features to the standard Markdown library. For this reason, the installation of this package is highly recommended as it's well-integrated with the Material theme.","title":"PyMdown Extensions"},{"location":"extensions/pymdown/#installation","text":"The PyMdown Extensions package can be installed with the following command: pip install pymdown-extensions The following list of extensions that are part of the PyMdown Extensions package are recommended to be used together with the Material theme: markdown_extensions : - pymdownx.arithmatex - pymdownx.betterem : smart_enable : all - pymdownx.caret - pymdownx.critic - pymdownx.details - pymdownx.emoji : emoji_generator : !!python/name:pymdownx.emoji.to_svg - pymdownx.inlinehilite - pymdownx.magiclink - pymdownx.mark - pymdownx.smartsymbols - pymdownx.superfences - pymdownx.tasklist : custom_checkbox : true - pymdownx.tilde","title":"Installation"},{"location":"extensions/pymdown/#usage","text":"","title":"Usage"},{"location":"extensions/pymdown/#arithmatex-mathjax","text":"Arithmatex integrates Material with MathJax which parses block-style and inline equations written in TeX markup and outputs them in mathematical notation. See this thread for a short introduction and quick reference on how to write equations in TeX syntax. Besides activating the extension in the mkdocs.yml , the MathJax JavaScript runtime needs to be included. This must be done with additional JavaScript : extra_javascript : - https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML If you want to override the default MathJax configuration, you can do this by adding another JavaScript file before the MathJax runtime in extra_javascript which contains your MathJax configuration, e.g.: window . MathJax = { tex2jax : { inlineMath : [ [ \\\\( , \\\\) ] ], displayMath : [ [ \\\\[ , \\\\] ] ] }, TeX : { TagSide : right , TagIndent : .8em , MultLineWidth : 85% , equationNumbers : { autoNumber : AMS , }, unicode : { fonts : STIXGeneral, Arial Unicode MS } }, displayAlign : left , showProcessingMessages : false , messageStyle : none }; In your mkdocs.yml , include it with: extra_javascript : - javascripts/extra.js - https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML","title":"Arithmatex MathJax"},{"location":"extensions/pymdown/#blocks","text":"Blocks are enclosed in $$ ... $$ which are placed on separate lines. Example: $$ \\frac {n ! }{k !( n - k )! } = \\binom {n}{k} $$ Result: \\frac{n!}{k!(n-k)!} = \\binom{n}{k} \\frac{n!}{k!(n-k)!} = \\binom{n}{k}","title":"Blocks"},{"location":"extensions/pymdown/#inline","text":"Inline equations need to be enclosed in $ ... $ : Example: Lorem ipsum dolor sit amet: $ p ( x|y ) = \\frac {p ( y|x ) p ( x ) }{p ( y ) } $ Result: Lorem ipsum dolor sit amet: p(x|y) = \\frac{p(y|x)p(x)}{p(y)} p(x|y) = \\frac{p(y|x)p(x)}{p(y)}","title":"Inline"},{"location":"extensions/pymdown/#betterem","text":"BetterEm improves the handling of emphasis markup ( bold and italic ) within Markdown by providing a more sophisticated parser for better detecting start and end tokens. Read the documentation for usage notes .","title":"BetterEm"},{"location":"extensions/pymdown/#caret","text":"Caret makes it possible to highlight inserted text . The portion of text that should be marked as added must be enclosed in two carets ^^...^^ .","title":"Caret"},{"location":"extensions/pymdown/#critic","text":"Critic implements Critic Markup , a Markdown extension that enables the tracking of changes (additions, deletions and comments) on documents. During compilation of the Markdown document, changes can be rendered (default), accepted or rejected. Text can be deleted and replacement text added . This can also be combined into one a single operation. Highlighting is also possible and comments can be added inline . Formatting can also be applied to blocks, by putting the opening and closing tags on separate lines and adding new lines between the tags and the content.","title":"Critic"},{"location":"extensions/pymdown/#details","text":"Details adds collapsible Admonition-style blocks which can contain arbitrary content using the HTML5 details and summary tags. Additionally, all Admonition qualifiers can be used, e.g. note , question , warning etc.: How many Prolog programmers does it take to change a lightbulb? Yes.","title":"Details"},{"location":"extensions/pymdown/#emoji","text":"Emoji adds the ability to insert a -load of emojis that we use in our daily lives. See the EmojiOne demo for a list of all available emojis. Happy scrolling Legal disclaimer Material has no affiliation with EmojiOne which is released under CC BY 4.0 . When including EmojiOne images or CSS, please read the EmojiOne license to ensure proper usage and attribution.","title":"Emoji"},{"location":"extensions/pymdown/#inlinehilite","text":"InlineHilite adds support for inline code highlighting. It's useful for short snippets included within body copy, e.g. var test = 0 ; and can be achived by prefixing inline code with a shebang and language identifier, e.g. #!js .","title":"InlineHilite"},{"location":"extensions/pymdown/#magiclink","text":"MagicLink detects links in Markdown and auto-generates the necessary markup, so no special syntax is required. It auto-links http[s]:// and ftp:// links, as well as references to email addresses.","title":"MagicLink"},{"location":"extensions/pymdown/#mark","text":"Mark adds the ability to highlight text like it was marked with a text marker . The portion of text that should be highlighted must be enclosed in two equal signs ==...== .","title":"Mark"},{"location":"extensions/pymdown/#smartsymbols","text":"SmartSymbols converts markup for special characters into their corresponding symbols, e.g. arrows ( , , ), trademark and copyright symbols ( , , ) and fractions ( , , ...).","title":"SmartSymbols"},{"location":"extensions/pymdown/#superfences","text":"SuperFences provides the ability to nest code blocks under blockquotes, lists and other block elements, which the Fenced Code Blocks extension from the standard Markdown library doesn't parse correctly. SuperFences does also allow grouping code blocks with tabs .","title":"SuperFences"},{"location":"extensions/pymdown/#tasklist","text":"Tasklist adds support for styled checkbox lists. This is useful for keeping track of tasks and showing what has been done and has yet to be done. Checkbox lists are like regular lists, but prefixed with [ ] for empty or [x] for filled checkboxes. Example: * [x] Lorem ipsum dolor sit amet, consectetur adipiscing elit * [x] Nulla lobortis egestas semper * [x] Curabitur elit nibh, euismod et ullamcorper at, iaculis feugiat est * [ ] Vestibulum convallis sit amet nisi a tincidunt * [x] In hac habitasse platea dictumst * [x] In scelerisque nibh non dolor mollis congue sed et metus * [x] Sed egestas felis quis elit dapibus, ac aliquet turpis mattis * [ ] Praesent sed risus massa * [ ] Aenean pretium efficitur erat, donec pharetra, ligula non scelerisque * [ ] Nulla vel eros venenatis, imperdiet enim id, faucibus nisi Result: Lorem ipsum dolor sit amet, consectetur adipiscing elit Nulla lobortis egestas semper Curabitur elit nibh, euismod et ullamcorper at, iaculis feugiat est Vestibulum convallis sit amet nisi a tincidunt In hac habitasse platea dictumst In scelerisque nibh non dolor mollis congue sed et metus Sed egestas felis quis elit dapibus, ac aliquet turpis mattis Praesent sed risus massa Aenean pretium efficitur erat, donec pharetra, ligula non scelerisque Nulla vel eros venenatis, imperdiet enim id, faucibus nisi","title":"Tasklist"},{"location":"extensions/pymdown/#tilde","text":"Tilde provides an easy way to strike through cross out text. The portion of text that should be erased must be enclosed in two tildes ~~...~~ and the extension will take care of the rest.","title":"Tilde"}]}